{
  
    
        "post0": {
            "title": "Regression - predicting house prices",
            "content": "Summary . The aim of this project is to experiment with data manipulation, adequateley prepare a dataset for use and apply various regression models to the manipulated data in order to build an accurate regression model. I also employed hyperparameter tuning to the best model to further increase performance. . All in all, I developed a regression model with an R2 score of ~0.87 . import numpy as np import pandas as pd import sklearn as sk from sklearn import preprocessing import matplotlib.pyplot as plt from sklearn import linear_model from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.neighbors import KNeighborsRegressor from sklearn.tree import DecisionTreeRegressor import seaborn as sns from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score import warnings from sklearn.svm import LinearSVR from sklearn.preprocessing import StandardScaler from sklearn import svm, datasets from sklearn.model_selection import GridSearchCV from sklearn.inspection import permutation_importance warnings.filterwarnings(action=&#39;ignore&#39;) pd.set_option(&quot;max_columns&quot;, 25) . . Data input and data cleaning . The first stage is to use pandas to read in the dataset into a dataframe, investigate the data, and replace any null values with 0&#39;s. . df = pd.read_csv(&#39;../projects/regression/kc_house_data.csv&#39;) df.fillna(0, inplace=True) df . id date price bedrooms bathrooms sqft_living sqft_lot floors waterfront view condition grade sqft_above sqft_basement yr_built yr_renovated zipcode lat long sqft_living15 sqft_lot15 . 0 7129300520 | 20141013T000000 | 221900.0 | 3 | 1.00 | 1180 | 5650 | 1.0 | 0 | 0 | 3 | 7 | 1180 | 0 | 1955 | 0 | 98178 | 47.5112 | -122.257 | 1340 | 5650 | . 1 6414100192 | 20141209T000000 | 538000.0 | 3 | 2.25 | 2570 | 7242 | 2.0 | 0 | 0 | 3 | 7 | 2170 | 400 | 1951 | 1991 | 98125 | 47.7210 | -122.319 | 1690 | 7639 | . 2 5631500400 | 20150225T000000 | 180000.0 | 2 | 1.00 | 770 | 10000 | 1.0 | 0 | 0 | 3 | 6 | 770 | 0 | 1933 | 0 | 98028 | 47.7379 | -122.233 | 2720 | 8062 | . 3 2487200875 | 20141209T000000 | 604000.0 | 4 | 3.00 | 1960 | 5000 | 1.0 | 0 | 0 | 5 | 7 | 1050 | 910 | 1965 | 0 | 98136 | 47.5208 | -122.393 | 1360 | 5000 | . 4 1954400510 | 20150218T000000 | 510000.0 | 3 | 2.00 | 1680 | 8080 | 1.0 | 0 | 0 | 3 | 8 | 1680 | 0 | 1987 | 0 | 98074 | 47.6168 | -122.045 | 1800 | 7503 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 21608 263000018 | 20140521T000000 | 360000.0 | 3 | 2.50 | 1530 | 1131 | 3.0 | 0 | 0 | 3 | 8 | 1530 | 0 | 2009 | 0 | 98103 | 47.6993 | -122.346 | 1530 | 1509 | . 21609 6600060120 | 20150223T000000 | 400000.0 | 4 | 2.50 | 2310 | 5813 | 2.0 | 0 | 0 | 3 | 8 | 2310 | 0 | 2014 | 0 | 98146 | 47.5107 | -122.362 | 1830 | 7200 | . 21610 1523300141 | 20140623T000000 | 402101.0 | 2 | 0.75 | 1020 | 1350 | 2.0 | 0 | 0 | 3 | 7 | 1020 | 0 | 2009 | 0 | 98144 | 47.5944 | -122.299 | 1020 | 2007 | . 21611 291310100 | 20150116T000000 | 400000.0 | 3 | 2.50 | 1600 | 2388 | 2.0 | 0 | 0 | 3 | 8 | 1600 | 0 | 2004 | 0 | 98027 | 47.5345 | -122.069 | 1410 | 1287 | . 21612 1523300157 | 20141015T000000 | 325000.0 | 2 | 0.75 | 1020 | 1076 | 2.0 | 0 | 0 | 3 | 7 | 1020 | 0 | 2008 | 0 | 98144 | 47.5941 | -122.299 | 1020 | 1357 | . 21613 rows × 21 columns . Next, I wanted simplify the &#39;date&#39; feature to just capture the year the house was recorded, and then use this in conjunction with the &#39;year_built&#39; feature to create one new feature; &#39;age&#39;. . d =[] for i in df[&#39;date&#39;].values: d.append(i[:4]) df[&#39;date&#39;] = d df[&#39;date&#39;]=df[&#39;date&#39;].astype(float) df[&#39;age&#39;] = df[&#39;date&#39;] - df[&#39;yr_built&#39;] . The next stage in data celaning was to remove any features which were obviously redundant. Here, I removed the &#39;date&#39; and &#39;yr_built&#39; functions which I had already simplified, alongside the &#39;id&#39; feature. . df = df.drop([&quot;date&quot;, &quot;id&quot;, &#39;yr_built&#39;], axis=1) . Removing correlated features . Next, I wrote some code to remove pairs of features which showed a high degree of correlation (&gt;80%). I did this as my understanding is that for regression problems, multicollinearity is bad. . After doing some more research, it seems that multicollinearity does not fundamentally worsen a model&#39;s peformance, but rather makes interpretation of coefficients/analysis of the features much harder. . I actually found a slight decrease in peformance when removing the correlated features, so for optimal peformance this code can be commented out. I have kept the code in case I ever want to dig deeper into interpreting the influence of certain features on the model. . # for i , r in df.corr().iterrows(): # k=0 # for j in range(len(r)): # if i!= r.index[k]: # if r.values[k] &gt;=0.5: # corr_features.append([i, r.index[k], r.values[k]]) # k += 1 # feat =[] # for i in corr_features: # if i[2] &gt;= 0.8: # feat.append(i[0]) # feat.append(i[1]) # df.drop(list(set(feat)), axis=1, inplace=True) . Visualise the distributions of each feature . At this stage, I thought it would be useful to have a look at what each feature&#39;s distribution looks like. . df.hist(figsize=(30,20)) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-10T17:12:22.686857 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ Removing outliers . I was conflicted about including this code. On one hand, removing outliers definitely improves the model&#39;s peformance. However, I am also of the opinion that almost all data holds value, so throwing some away doesn&#39;t sit great with me. . In the end I became a sucker for peformance, and decided to remove the outliers from the datasets. Beyond just peformance gains, there is rationale for doing this. Regressions are designed to summarise the data, and having outliers greatly swing the summary of the vast majority of the dataset isn&#39;t always logical. Sometimes new, outlying data points won&#39;t be described well by a model that has omitted outliers, but it should boost peformance in the general case. . In summary, this issue is something that needs to be considered in a case by case basis. As long as there is explained reason behind the decision to remove outliers (or not), then there is good justification to go either way based on the circumstances. . In terms of implementation, as the price follows a somewhat gaussian distribution, I used a standard deviation cut-off approach to removing outliers. In short, each price was normalized, and if this value was greater than or less than 3 then I removed that row from the dataset. I&#39;ve also visualised the outliers, where blue is low outliers and red is high outliers. . zscore = [] outlier =[] threshold = 3 price_mean = np.mean(df[&#39;price&#39;]) price_std = np.std(df[&#39;price&#39;]) for i in df[&#39;price&#39;]: z = (i-price_mean)/price_std zscore.append(z) if np.abs(z) &gt; threshold: outlier.append(i) plt.figure(figsize = (10,6)) sns.distplot(zscore, kde=False) plt.axvspan(xmin = -3 ,xmax= min(zscore),alpha=0.2, color=&#39;blue&#39;, label=&#39;Lower Outliers&#39;) plt.axvspan(xmin = 3 ,xmax= max(zscore),alpha=0.2, color=&#39;red&#39;, label=&#39;Upper Outliers&#39;) plt.show() dj=[] for i in df.price: if i in set(outlier): dj.append(0.0) else: dj.append(i) df[&#39;P&#39;] = dj df = df.drop(df[df[&#39;P&#39;] == 0.0].index) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-10T17:12:27.551570 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ After the outliers were removed, I split the dataset into a feature set and a price set, and removed the redundant features appropriatley. . X = df.drop([&#39;price&#39;,&#39;P&#39;], axis=1) Y = df[&#39;price&#39;] . Preparing the training and test datasets . The first step was to easily split the dataframes into train and test sets, on a 70:30 ratio. . x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3) . Next, to deal with categorical values (only zipcode in this case), I employed one-hot encoding to categorical features in the feature sets. . train_zip = pd.get_dummies(x_train.zipcode, prefix=&quot;zipcode&quot;) x_train = x_train.drop(&#39;zipcode&#39;,axis=1) x_train = x_train.join(train_zip) test_zip = pd.get_dummies(x_test.zipcode, prefix=&quot;zipcode&quot;) x_test = x_test.drop(&#39;zipcode&#39;,axis=1) x_test = x_test.join(test_zip) . The final stage before model selection was to scale the data. I went for the StandardScaler, which implements a basic scaling based off the mean and standard deviation of the datasets. . It is important to note that the scaling fit to the training set is the same scaling applied to the test set. This is to avoid the model learning anything about the test set before we let it attack it. . scaler = StandardScaler() scaler.fit(x_train) scaler.transform(x_train) scaler.transform(x_test) . array([[-0.38294392, 0.22078998, -0.03385286, ..., -0.07766501, -0.11745607, -0.12151626], [-0.38294392, -1.47946242, -0.53574937, ..., -0.07766501, -0.11745607, -0.12151626], [-0.38294392, -0.45931098, 0.43219389, ..., -0.07766501, -0.11745607, -0.12151626], ..., [-0.38294392, -0.1192605 , -0.24895137, ..., -0.07766501, -0.11745607, -0.12151626], [-0.38294392, -1.47946242, -1.12129529, ..., -0.07766501, -0.11745607, -0.12151626], [-0.38294392, 2.26109287, 2.0095829 , ..., -0.07766501, -0.11745607, -0.12151626]]) . . Model Selection . To start I defined a set of models which I wanted to include in my model evaluation. These included the ones I was most familiar with, or had heard good things about. . lr = linear_model.LinearRegression() ridge = linear_model.Ridge() lasso = linear_model.Lasso() e_net = linear_model.ElasticNet() r_forest = RandomForestRegressor(n_jobs=-1) k_neighbours = KNeighborsRegressor() d_tree = DecisionTreeRegressor() models = [lr, ridge, lasso, e_net, r_forest, k_neighbours, d_tree] model_names = [&#39;Linear&#39;, &#39;Ridge&#39;, &#39;Lasso&#39;, &#39;Elastic Net&#39;, &#39;Random Forest&#39;, &#39;KNN&#39;, &#39;Decision Tree&#39;] . Next, I created a set of cross-validation sets via the sklearn KFold validator class. This allowed me to generate a preliminary R2 score for each model via cross-validation, without having to fully test each model (which would have taken an age!). . for i in range(len(models)): kfold = KFold(n_splits=7) result = cross_val_score(models[i], x_train, y_train, cv=kfold, scoring=&#39;r2&#39;) print(model_names[i] + &#39;: &#39; + str(result.mean()) ) . Linear: 0.834841451524125 Ridge: 0.834788741056295 Lasso: 0.8348479388140282 Elastic Net: 0.6154086881998547 Random Forest: 0.868900272979691 KNN: 0.4468420436933553 Decision Tree: 0.742939379397493 . Hyperparameter tuning . Random Forest Regression came out top, with the highest R2 score. As such, in the intertest of time, I made the assumption that once tuned, this would still be the highest peforming model. . In this step, I chose the three most prudent parameters which I thought would affect the peformance of the model; number of trees, criterion (function which measures the quality of a split) and number of features considered when looking for optimal splits. . parameters = { &#39;n_estimators&#39;: [10, 50, 100], &#39;criterion&#39;: [&#39;mse&#39;, &#39;mae&#39;], &#39;max_features&#39;: [&#39;auto&#39;, &#39;sqrt&#39;, &#39;log2&#39;], } . Next, I used GridSearchCV to find the optimal set of my selected parameters; 200, mae and auto respectivley. Admittedly the first of these, number of estimators, was always going to be the largest value, so this could have been omitted from the search. If I wanted to optimise efficiency I could have plotted the peformance as a function of the number of estimators and found the point at which peformance gains become minimal. . grid = GridSearchCV(r_forest, param_grid = parameters,n_jobs=-1, scoring=&#39;r2&#39;, verbose=2) grid.fit(x_train, y_train) print(grid.best_score_) print(grid.best_params_) . Fitting 5 folds for each of 18 candidates, totalling 90 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers. [Parallel(n_jobs=-1)]: Done 25 tasks | elapsed: 13.6s [Parallel(n_jobs=-1)]: Done 90 out of 90 | elapsed: 23.0min finished 0.8686275134918929 {&#39;criterion&#39;: &#39;mse&#39;, &#39;max_features&#39;: &#39;auto&#39;, &#39;n_estimators&#39;: 100} . Final Model . Now that I have selected the optimal model and tuned its parameters, its a simple process to train and use the model on the dataset. I&#39;ve printed the R2 score here for completeness. . Note that I have included an extra parameter &#39;n_jobs&#39; which defines how many concurrent processes to run; -1 uses all available CPU cores so shoud speed it up! . r_forest = RandomForestRegressor(criterion=&#39;mae&#39;, max_features=&#39;auto&#39;, n_estimators=100,n_jobs=-1) r_forest.fit(x_train,y_train) predictions = r_forest.predict(x_test) score = r2_score(y_test,predictions) print(score) . 0.8677027918004787 . Finally, I can use the model to make a new price prediction of a new dummy data point (for ease I have made a new dummy point that is the average values across all the testing set data points). . new_pred = r_forest.predict([np.array(x_test).mean(axis=0)]) print(new_pred) . [593140.] . Feature Importance . As an added bonus, I have taken a look at how significant each feature was in the model. Firstly, I plotted all feature importances, and saw that the zipcode features (which were numerous, at the end of the feature list) had low significance. . feature_importances = r_forest.feature_importances_ plt.bar([x for x in range(len(feature_importances))], feature_importances) . &lt;BarContainer object of 87 artists&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-10T17:59:39.157283 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ Removing these from the chart allows us a better view of feature significance. . importances_no_zips = feature_importances[0:17] relevant_features = df.columns.tolist() relevant_features.remove(&#39;id&#39;) relevant_features.remove(&#39;date&#39;) relevant_features.remove(&#39;price&#39;) relevant_features.remove(&#39;yr_built&#39;) relevant_features.remove(&#39;zipcode&#39;) relevant_features.append(&#39;age&#39;) fig = plt.figure(figsize=[30,10]) plt.bar(relevant_features, importances_no_zips) . &lt;BarContainer object of 17 artists&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-10T17:59:41.677306 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ This shows a relativley high importance for the living space feature, the grade feature, and , surpisingly, the latitude feature. This being said, geographical location is a big driver of house prices, and tree based regression methods would use longitude and latitude effectivley to predict pricing. . Linear regressions and other regression models may focus on other features, and their importances could be derived in a similar way to the above if different models were chosen. . Conclusion . In this notebook I have experimented with regression models, and found an optimal model to use on the housing dataset. The process included data manipulation/feature selection, oulier removal, correlated feature removal, feature scaling, cross-validation to select the best type of model, hyperparameter tuning, utilisation of the tuned model and finally, an analysis of feature significance. . Final R2 scores sat at roughly 0.87. To improve this I could look into further feature selection, or perhaps dimensionality reduction. There is also the prospect of utilising a neural network, but that may be overkill here and I cannot iamgine there would be a significant peformance gain. . I welcome any feedback, comments or suggestions for improvement! .",
            "url": "https://ah161652.github.io/data_science_projects/regression/2020/06/09/Regression.html",
            "relUrl": "/regression/2020/06/09/Regression.html",
            "date": " • Jun 9, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ah161652.github.io/data_science_projects/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ah161652.github.io/data_science_projects/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ah161652.github.io/data_science_projects/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}