{
  
    
        "post0": {
            "title": "Classification - heart disease",
            "content": "Summary . The aim of this project is to experiment with data manipulation, adequately prepare a dataset for use and apply various classification models to the manipulated data in order to build an accurate classification model. I also employed hyperparameter tuning to the best model to further increase performance. . All in all, I developed a classification model with accuracy and F1 scores both around 0.83. The recall score, which is particulary important for medical classification models, sat at 0.91. . Libraries . import numpy as np import pandas as pd import sklearn as sk from sklearn import preprocessing import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn import linear_model from sklearn import naive_bayes from sklearn import neighbors from sklearn import tree from sklearn import ensemble from sklearn import svm from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score import warnings from sklearn import metrics from sklearn.model_selection import GridSearchCV warnings.filterwarnings(action=&#39;ignore&#39;) pd.set_option(&quot;max_columns&quot;, 25) . . . Data input and exploration . Firstly, I read in the dataset and had a little explore of the features. This was a relativley small dataset of 303 entries, with 13 features contributing to the classification of either no heart disease (0) or heart disease (1). It was also good to see that the dataset was complete, with no null values across all entries, so there was no need for dealing with any null values. . df = pd.read_csv(&quot;../projects/Classification/heart.csv&quot;) . df.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 303 entries, 0 to 302 Data columns (total 14 columns): # Column Non-Null Count Dtype -- -- 0 age 303 non-null int64 1 sex 303 non-null int64 2 cp 303 non-null int64 3 trestbps 303 non-null int64 4 chol 303 non-null int64 5 fbs 303 non-null int64 6 restecg 303 non-null int64 7 thalach 303 non-null int64 8 exang 303 non-null int64 9 oldpeak 303 non-null float64 10 slope 303 non-null int64 11 ca 303 non-null int64 12 thal 303 non-null int64 13 target 303 non-null int64 dtypes: float64(1), int64(13) memory usage: 33.3 KB . Next, I wanted to visualise the distribution of the features. As you can see, there are a significant number of categorical features, which I will deal with later. For completeness, see below for the full description of the features: . age: Self explanatory | sex: 0 is female, 1 is male | cp: Chest pain type in increasing severity | trestbps: Resting blood pressure upon hospital admission | chol: Cholesterol levels | fbs: Was the patient fasting (1 = true, 0 = false) | restecg: Category of resting electrocardiograph results | thalach: Maximum heart rate | exang: Does the patient suffer from exercise induced angina (1 = true, 0 = false) | oldpeak: ST depression induced by exercise | slope: The slope of the peak exercise ST segment | ca: Number of major vessels (0-3) colored by fluoroscopy | thal: Category of a blood disorder called thalassemia, where 3 is normal, and 0-2 are abnormal. | target: Heart disease or not (1 = true, 0 = false) | . df.hist(figsize=(30,20)) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-26T09:59:09.487589 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ I also wanted to get a feel for if there was any inbalance in the dependent variable. As seen below, the dataset is slightly inbalanced, but not the to the degree that I should be concerned that my models will not be effective. . sns.countplot(x=&quot;target&quot;, data=df) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-26T10:18:43.947607 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ . Data cleaning . As the dataset was already in very good condition, the only additional cleaning I could think to do was to check for any correlated features and remove them from the model. I wrote some code to remove pairs of features which showed a high degree of correlation (&gt;80%). I did this as my understanding is that highly correlated features are redundant in many classification algorithms, and are detrimental for some. . It turned out that none of the features were highly correlated, so this code is redundant, but I have kept it in for reference. I have also printed a matrix of correlation, which proves my code is correct! . corr_features =[] for i , r in df.corr().iterrows(): k=0 for j in range(len(r)): if i!= r.index[k]: if r.values[k] &gt;=0.5: corr_features.append([i, r.index[k], r.values[k]]) k += 1 feat =[] for i in corr_features: print(i[2]) if i[2] &gt;= 0.8: feat.append(i[0]) feat.append(i[1]) df.drop(list(set(feat)), axis=1, inplace=True) . df.corr() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . age 1.000000 | -0.098447 | -0.068653 | 0.279351 | 0.213678 | 0.121308 | -0.116211 | -0.398522 | 0.096801 | 0.210013 | -0.168814 | 0.276326 | 0.068001 | -0.225439 | . sex -0.098447 | 1.000000 | -0.049353 | -0.056769 | -0.197912 | 0.045032 | -0.058196 | -0.044020 | 0.141664 | 0.096093 | -0.030711 | 0.118261 | 0.210041 | -0.280937 | . cp -0.068653 | -0.049353 | 1.000000 | 0.047608 | -0.076904 | 0.094444 | 0.044421 | 0.295762 | -0.394280 | -0.149230 | 0.119717 | -0.181053 | -0.161736 | 0.433798 | . trestbps 0.279351 | -0.056769 | 0.047608 | 1.000000 | 0.123174 | 0.177531 | -0.114103 | -0.046698 | 0.067616 | 0.193216 | -0.121475 | 0.101389 | 0.062210 | -0.144931 | . chol 0.213678 | -0.197912 | -0.076904 | 0.123174 | 1.000000 | 0.013294 | -0.151040 | -0.009940 | 0.067023 | 0.053952 | -0.004038 | 0.070511 | 0.098803 | -0.085239 | . fbs 0.121308 | 0.045032 | 0.094444 | 0.177531 | 0.013294 | 1.000000 | -0.084189 | -0.008567 | 0.025665 | 0.005747 | -0.059894 | 0.137979 | -0.032019 | -0.028046 | . restecg -0.116211 | -0.058196 | 0.044421 | -0.114103 | -0.151040 | -0.084189 | 1.000000 | 0.044123 | -0.070733 | -0.058770 | 0.093045 | -0.072042 | -0.011981 | 0.137230 | . thalach -0.398522 | -0.044020 | 0.295762 | -0.046698 | -0.009940 | -0.008567 | 0.044123 | 1.000000 | -0.378812 | -0.344187 | 0.386784 | -0.213177 | -0.096439 | 0.421741 | . exang 0.096801 | 0.141664 | -0.394280 | 0.067616 | 0.067023 | 0.025665 | -0.070733 | -0.378812 | 1.000000 | 0.288223 | -0.257748 | 0.115739 | 0.206754 | -0.436757 | . oldpeak 0.210013 | 0.096093 | -0.149230 | 0.193216 | 0.053952 | 0.005747 | -0.058770 | -0.344187 | 0.288223 | 1.000000 | -0.577537 | 0.222682 | 0.210244 | -0.430696 | . slope -0.168814 | -0.030711 | 0.119717 | -0.121475 | -0.004038 | -0.059894 | 0.093045 | 0.386784 | -0.257748 | -0.577537 | 1.000000 | -0.080155 | -0.104764 | 0.345877 | . ca 0.276326 | 0.118261 | -0.181053 | 0.101389 | 0.070511 | 0.137979 | -0.072042 | -0.213177 | 0.115739 | 0.222682 | -0.080155 | 1.000000 | 0.151832 | -0.391724 | . thal 0.068001 | 0.210041 | -0.161736 | 0.062210 | 0.098803 | -0.032019 | -0.011981 | -0.096439 | 0.206754 | 0.210244 | -0.104764 | 0.151832 | 1.000000 | -0.344029 | . target -0.225439 | -0.280937 | 0.433798 | -0.144931 | -0.085239 | -0.028046 | 0.137230 | 0.421741 | -0.436757 | -0.430696 | 0.345877 | -0.391724 | -0.344029 | 1.000000 | . . Preparing the train/test sets and dealing with categorical variables . First, I needed to deal with all the categorical features in the dataset. These were the sex, fbs, restecg, exang, slope and thal features. Note that the cp and ca features look categorical, but they are ordinal, as they have a natural ordering (increasing severity of chest pain and number of highlighted blood vessels) so I have kept these as continious variables, albeit accross a small range. . I used one-hot encoding to split each categorical feature into a series of new binary features which are more easily interpreted by many models. . x = df.drop([&#39;target&#39;], axis=1) y = df[&#39;target&#39;] cat_features_string = [&#39;sex&#39;,&#39;fbs&#39;,&#39;restecg&#39;,&#39;exang&#39;,&#39;slope&#39;,&#39;thal&#39;] cat_features = [x.sex,x.fbs,x.restecg,x.exang,x.slope,x.thal] for i in range(len(cat_features_string)): x_temp = pd.get_dummies(cat_features[i], prefix=cat_features_string[i]) x = x.drop(cat_features_string[i],axis=1) x = x.join(x_temp) . Once this was done it was a simple matter of splitting the dataframes into train an test sets, at a 70/30 split. . x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3) . x_train.head() . age cp trestbps chol thalach oldpeak ca sex_0 sex_1 fbs_0 fbs_1 restecg_0 restecg_1 restecg_2 exang_0 exang_1 slope_0 slope_1 slope_2 thal_0 thal_1 thal_2 thal_3 . 95 53 | 0 | 142 | 226 | 111 | 0.0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | . 156 47 | 2 | 130 | 253 | 179 | 0.0 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | . 169 53 | 0 | 140 | 203 | 155 | 3.1 | 0 | 0 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | . 206 59 | 0 | 110 | 239 | 142 | 1.2 | 1 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | . 81 45 | 1 | 128 | 308 | 170 | 0.0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | . . Data normalisation . The final bit of data manipulation to do was to scale the data, via the StandardScaler class. It is important to note that the scaling fit to the training set is the same scaling applied to the test set. This is to avoid the model learning anything about the test set before we let it attack it. . scaler = StandardScaler() scaler.fit(x_train) scaler.transform(x_train) scaler.transform(x_test) . array([[-0.58533247, -0.0089799 , -0.01274095, ..., -0.24494897, 0.95389198, -0.83426614], [-1.15143869, -0.96084962, 0.10469735, ..., -0.24494897, -1.04833673, 1.19865825], [ 1.56587117, 0.94288982, 1.2790803 , ..., -0.24494897, 0.95389198, -0.83426614], ..., [ 0.66010122, -0.96084962, 2.33602496, ..., -0.24494897, -1.04833673, 1.19865825], [ 1.45264993, -0.96084962, 1.74883348, ..., 4.0824829 , -1.04833673, -0.83426614], [ 1.45264993, -0.96084962, -1.06968561, ..., -0.24494897, 0.95389198, -0.83426614]]) . . . Model selection . The first thing to do was to decide which classification models I was going to experiment with. I have taken a selection of models which I am familiar with, or have heard of. . log_regression = linear_model.LogisticRegression() n_bayes = naive_bayes.GaussianNB() sgd = linear_model.SGDClassifier() knn = neighbors.KNeighborsClassifier() dec_tree = tree.DecisionTreeClassifier() rand_forest = ensemble.RandomForestClassifier(n_jobs=-1) svc = svm.SVC() models = [log_regression, n_bayes, sgd, knn, dec_tree, rand_forest, svc] model_names = [&#39;Logistic Regression&#39;, &#39;Naive Bayes&#39;, &#39;Stochastic Gradient Descent&#39;, &#39;K-Nearest-Neighbours&#39;, &#39;Decision Tree&#39;, &#39;Random Forest&#39;, &#39;Support Vector Classifier&#39;] . Next, I created a set of cross-validation sets via the sklearn KFold validator class. This allowed me to generate preliminary scores for each model via cross-validation, without having to fully test each model. . I have chosen to evaluate my model with 4 scores; accuracy, F1 score, recall and precision. This is because accuracy is not always the best indicator of an effective classification model. The F1 score gives a balanced view of both precision and recall, but I have included recall and precision scores also, for completeness. . As this is a medical classification model, we want to see a high recall rate in order to keep the false negative rates low, which can be dangerous in medical settings. If we can get a model that also has high accuracy and F1 scores, thereby reducing the number of false positives too, then even better! . print(&#39;Recall:&#39;) for i in range(len(models)): kfold = KFold(n_splits=7) result = cross_val_score(models[i], x_train, y_train, cv=kfold, scoring=&#39;recall&#39;) print(model_names[i] + &#39;: &#39; + str(result.mean()) ) print(&#39;Precision:&#39;) for i in range(len(models)): kfold = KFold(n_splits=7) result = cross_val_score(models[i], x_train, y_train, cv=kfold, scoring=&#39;precision&#39;) print(model_names[i] + &#39;: &#39; + str(result.mean()) ) print(&#39;F1:&#39;) for i in range(len(models)): kfold = KFold(n_splits=7) result = cross_val_score(models[i], x_train, y_train, cv=kfold, scoring=&#39;f1&#39;) print(model_names[i] + &#39;: &#39; + str(result.mean()) ) print(&#39;Accuracy:&#39;) for i in range(len(models)): kfold = KFold(n_splits=7) result = cross_val_score(models[i], x_train, y_train, cv=kfold, scoring=&#39;accuracy&#39;) print(model_names[i] + &#39;: &#39; + str(result.mean()) ) . Recall: Logistic Regression: 0.8800924013330027 Naive Bayes: 0.8129856463503079 Stochastic Gradient Descent: 0.4858560090702948 K-Nearest-Neighbours: 0.7308858226151458 Decision Tree: 0.7930175714010301 Random Forest: 0.8700867323987624 Support Vector Classifier: 0.9254385964912281 Precision: Logistic Regression: 0.8315786638542056 Naive Bayes: 0.8493772309561782 Stochastic Gradient Descent: 0.7714285714285715 K-Nearest-Neighbours: 0.6587292093871041 Decision Tree: 0.7772203091763021 Random Forest: 0.8021119986456121 Support Vector Classifier: 0.6293854696028609 F1: Logistic Regression: 0.8525817392164293 Naive Bayes: 0.8255877541591826 Stochastic Gradient Descent: 0.570452918009309 K-Nearest-Neighbours: 0.683342810808534 Decision Tree: 0.7841796462486118 Random Forest: 0.8277212205783634 Support Vector Classifier: 0.7402477037445304 Accuracy: Logistic Regression: 0.8394777265745006 Naive Bayes: 0.8109062980030721 Stochastic Gradient Descent: 0.6125960061443932 K-Nearest-Neighbours: 0.6231950844854071 Decision Tree: 0.7540706605222735 Random Forest: 0.8204301075268816 Support Vector Classifier: 0.6367127496159755 . The results were interesting. If I ran the above code without changing anything then logistic regression came out on top, with an accuracy of 84 and an F1 score of 85. . If I removed the normalisation of the variables, I saw similar results. . If I removed the one-hot encoding but kept the normalisation the logistic regression classifier maintained its peformance, and was rivalled by naive bayes for both accuracy and F1 score. . If I removed normalisation and one hot encoding there was a slight hit to the peformance of the logistic regression model, and the naive bayes model came out on top with 82 accuracy and 84 F1. . As such, it was a toss up between naive bayes with no normalisaton or one-hot encoding, vs logistic regression with the above two steps enabled. I decided to stick with the code I had already written and make use of the logistic regression as this seemed to be the most consistent with some further repeated tests. . I believe the Naive Bayes classifier faltered with the one hot encoding enabled as I was using a gaussian Naive Bayes algorithm, which isn&#39;t best suited with alot of binary features. If I changed the Naive Bayes implementation to a Bernoulli Naive Bayes implementation then I received slightly better results, but still not as good as logisitic regression. I put this down to Bernoulli handling binary features well, but there are still plenty of non binary features in the dataset. . Final decision: Logistic regression classifier. . . Hyperparameter tuning . I made the assumption that once tuned, the logistic regression classifier would still be the highest peforming model. . In this step, I chose the most prudent parameters which I thought would affect the peformance of the model. . parameters = { &#39;penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;, &#39;elasticnet&#39;, &#39;none&#39;], &#39;solver&#39;: [&#39;newton-cg&#39;, &#39;lbfgs&#39;, &#39;liblinear&#39;, &#39;sag&#39;, &#39;saga&#39;], } . Next, I used GridSearchCV to find the optimal set of my selected parameters; this turned out to be the &#39;l1&#39; and &#39;liblinear&#39;. The scoring I was comapring against was recall, as for medical classification I want as high a recall score as possible. . grid = GridSearchCV(log_regression, param_grid = parameters,n_jobs=-1, scoring=&#39;recall&#39;, verbose=2) grid.fit(x_train, y_train) print(grid.best_score_) print(grid.best_params_) . Fitting 5 folds for each of 20 candidates, totalling 100 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers. [Parallel(n_jobs=-1)]: Done 34 tasks | elapsed: 0.2s [Parallel(n_jobs=-1)]: Done 85 out of 100 | elapsed: 0.3s remaining: 0.1s 0.8833333333333334 {&#39;penalty&#39;: &#39;l1&#39;, &#39;solver&#39;: &#39;liblinear&#39;} [Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 0.5s finished . . Final model . Now that I have selected the optimal model and tuned its parameters, its a simple process to train and use the model on the dataset. I&#39;ve printed the F1 and accuracy scores here for completeness. . log_regression = linear_model.LogisticRegression(penalty=&#39;l1&#39;, solver=&#39;liblinear&#39;, max_iter=1000000, n_jobs=-1) log_regression.fit(x_train,y_train) predictions = log_regression.predict(x_test) accuracy = metrics.accuracy_score(y_test,predictions) f1 = metrics.f1_score(y_test,predictions) recall = metrics.recall_score(y_test,predictions) precision = metrics.precision_score(y_test,predictions) print(&#39;Accuracy: &#39;+str(accuracy)) print(&#39;F1 Score: &#39;+str(f1)) print(&#39;Recall: &#39;+str(recall)) print(&#39;Precision: &#39;+str(precision)) . Accuracy: 0.8131868131868132 F1 Score: 0.8210526315789474 Recall: 0.8666666666666667 Precision: 0.78 . . Model evaluation . The model looks pretty good. 81% accuracy, F1 score of 0.82 and a nice high recall of 0.87, keeping the false negatives of heart disease down. . I&#39;ve visualised the confusion matrix and ROC plot below to see what the split of predictions is. . c_matrix = metrics.plot_confusion_matrix(log_regression, x_test,y_test) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-26T15:18:34.286067 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ metrics.plot_roc_curve(log_regression, x_test,y_test) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-26T15:10:16.988678 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ On the surface, the model looks to be peforming strongly. The accuracy is high, the F1 score is strong, and the AUC is close to 1. . This being said, theres still roughly 13% of positive cases being mislabelled as negative cases. This could be down to the small sample size, or could indicate that the model needs further tweaking in order to favour recall over precision. . . Increasing recall . In order to boost the recall value, I am going to experiment with the &#39;C&#39; parameter for logistic regression. This is the inverse of regularisation strength, with smaller values normally leading to a greater recall. The default is 1, so let&#39;s lower it as see what happens. . log_regression = linear_model.LogisticRegression(penalty=&#39;l1&#39;, solver=&#39;liblinear&#39;, max_iter=1000000, n_jobs=-1, C=0.25) log_regression.fit(x_train,y_train) predictions = log_regression.predict(x_test) accuracy = metrics.accuracy_score(y_test,predictions) f1 = metrics.f1_score(y_test,predictions) recall = metrics.recall_score(y_test,predictions) precision = metrics.precision_score(y_test,predictions) print(&#39;Accuracy: &#39;+str(accuracy)) print(&#39;F1 Score: &#39;+str(f1)) print(&#39;Recall: &#39;+str(recall)) print(&#39;Precision: &#39;+str(precision)) c_matrix = metrics.plot_confusion_matrix(log_regression, x_test,y_test) plt.show() . Accuracy: 0.8241758241758241 F1 Score: 0.8367346938775511 Recall: 0.9111111111111111 Precision: 0.7735849056603774 . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-26T15:23:34.200498 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ With the reduced C value I can get the recall to 91 and the false negative rate to under 10% of true negatives. That is pretty good for a normal classification model, but this certainly wouldn&#39;t be up to scratch for a true medical model! . To improve the model further I think that I would need to do some in depth feature engineering. This would require a level of medical domain knowledge which I do not posess, so for now I am going to accept my model as adequate. . . Feature analysis . Finally, I want to take a look at which features had the most signficance on the model. The simplest thing to do here is to take a look at the coefficients asscoiated with each feature. The higher the coefficient, the higher its influence on the model. . coefs = log_regression.coef_[0] features = x_train.columns.tolist() fig = plt.figure(figsize=[30,10]) plt.bar(features, coefs) . &lt;BarContainer object of 23 artists&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-26T15:34:20.462626 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ This led to some interesting insight. Firsly, age did not play a big part in the model, which may be of surprise. . Chest pain severity was a big factor, as expected, as was being female and having an abnormal thalassemia condition. . Maximum heartrate and number of major blood vessels visible under fluoroscopy were strongly weighted against heart disease, which does make sense, as was being a man and having a normal thalassemia condition. . These results seem reasonable, however I am not a doctor, nor do I have any background in medicine, so my interpretation of these results is somewhat limitted. That is why data science is so interesting, as if I was truly tasked with this problem then I would have to become clued up on the subject matter in order to analyse in depth. . . Conclusion . In this notebook I have experimented with classification models, and found an optimal model to use on the medical dataset. The process included data manipulation and cleaning, correlated feature removal, feature scaling, cross-validation to select the best type of model, hyperparameter tuning, utilisation of the tuned model, evaluation alongside visualisation and finally, an analysis of feature significance. . Final accuracy and F1 scores sat at roughly 0.83. I think this is close to the maximal peformance I can get out of this dataset without doing some more complex feature engineering, increasing the sample size, or using a more in depth feature set. . I welcome any feedback, comments or suggestions for improvement! .",
            "url": "https://ah161652.github.io/data_science_projects/classification/2021/06/26/Classification.html",
            "relUrl": "/classification/2021/06/26/Classification.html",
            "date": " • Jun 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Regression - predicting house prices",
            "content": "Summary . The aim of this project is to experiment with data manipulation, adequately prepare a dataset for use and apply various regression models to the manipulated data in order to build an accurate regression model. I also employed hyperparameter tuning to the best model to further increase performance. . All in all, I developed a regression model with an R2 score of ~0.87 . Full code can be found here. . . Library imports . import numpy as np import pandas as pd import sklearn as sk from sklearn import preprocessing import matplotlib.pyplot as plt from sklearn import linear_model from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.neighbors import KNeighborsRegressor from sklearn.tree import DecisionTreeRegressor import seaborn as sns from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score import warnings from sklearn.svm import LinearSVR from sklearn.preprocessing import StandardScaler from sklearn import svm, datasets from sklearn.model_selection import GridSearchCV from sklearn.inspection import permutation_importance warnings.filterwarnings(action=&#39;ignore&#39;) pd.set_option(&quot;max_columns&quot;, 25) . . . Data input and data cleaning . The first stage is to use pandas to read in the dataset into a dataframe, investigate the data, and replace any null values with 0&#39;s. . df = pd.read_csv(&#39;../projects/regression/kc_house_data.csv&#39;) df.fillna(0, inplace=True) df . id date price bedrooms bathrooms sqft_living sqft_lot floors waterfront view condition grade sqft_above sqft_basement yr_built yr_renovated zipcode lat long sqft_living15 sqft_lot15 . 0 7129300520 | 20141013T000000 | 221900.0 | 3 | 1.00 | 1180 | 5650 | 1.0 | 0 | 0 | 3 | 7 | 1180 | 0 | 1955 | 0 | 98178 | 47.5112 | -122.257 | 1340 | 5650 | . 1 6414100192 | 20141209T000000 | 538000.0 | 3 | 2.25 | 2570 | 7242 | 2.0 | 0 | 0 | 3 | 7 | 2170 | 400 | 1951 | 1991 | 98125 | 47.7210 | -122.319 | 1690 | 7639 | . 2 5631500400 | 20150225T000000 | 180000.0 | 2 | 1.00 | 770 | 10000 | 1.0 | 0 | 0 | 3 | 6 | 770 | 0 | 1933 | 0 | 98028 | 47.7379 | -122.233 | 2720 | 8062 | . 3 2487200875 | 20141209T000000 | 604000.0 | 4 | 3.00 | 1960 | 5000 | 1.0 | 0 | 0 | 5 | 7 | 1050 | 910 | 1965 | 0 | 98136 | 47.5208 | -122.393 | 1360 | 5000 | . 4 1954400510 | 20150218T000000 | 510000.0 | 3 | 2.00 | 1680 | 8080 | 1.0 | 0 | 0 | 3 | 8 | 1680 | 0 | 1987 | 0 | 98074 | 47.6168 | -122.045 | 1800 | 7503 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 21608 263000018 | 20140521T000000 | 360000.0 | 3 | 2.50 | 1530 | 1131 | 3.0 | 0 | 0 | 3 | 8 | 1530 | 0 | 2009 | 0 | 98103 | 47.6993 | -122.346 | 1530 | 1509 | . 21609 6600060120 | 20150223T000000 | 400000.0 | 4 | 2.50 | 2310 | 5813 | 2.0 | 0 | 0 | 3 | 8 | 2310 | 0 | 2014 | 0 | 98146 | 47.5107 | -122.362 | 1830 | 7200 | . 21610 1523300141 | 20140623T000000 | 402101.0 | 2 | 0.75 | 1020 | 1350 | 2.0 | 0 | 0 | 3 | 7 | 1020 | 0 | 2009 | 0 | 98144 | 47.5944 | -122.299 | 1020 | 2007 | . 21611 291310100 | 20150116T000000 | 400000.0 | 3 | 2.50 | 1600 | 2388 | 2.0 | 0 | 0 | 3 | 8 | 1600 | 0 | 2004 | 0 | 98027 | 47.5345 | -122.069 | 1410 | 1287 | . 21612 1523300157 | 20141015T000000 | 325000.0 | 2 | 0.75 | 1020 | 1076 | 2.0 | 0 | 0 | 3 | 7 | 1020 | 0 | 2008 | 0 | 98144 | 47.5941 | -122.299 | 1020 | 1357 | . 21613 rows × 21 columns . Next, I wanted simplify the &#39;date&#39; feature to just capture the year the house was recorded, and then use this in conjunction with the &#39;year_built&#39; feature to create one new feature; &#39;age&#39;. . d =[] for i in df[&#39;date&#39;].values: d.append(i[:4]) df[&#39;date&#39;] = d df[&#39;date&#39;]=df[&#39;date&#39;].astype(float) df[&#39;age&#39;] = df[&#39;date&#39;] - df[&#39;yr_built&#39;] . The next stage in data cleaning was to remove any features which were obviously redundant. Here, I removed the &#39;date&#39; and &#39;yr_built&#39; features which I had already simplified, alongside the &#39;id&#39; feature. . df = df.drop([&quot;date&quot;, &quot;id&quot;, &#39;yr_built&#39;], axis=1) . . Removing correlated features . Next, I wrote some code to remove pairs of features which showed a high degree of correlation (&gt;80%). I did this as my understanding is that for regression problems, multicollinearity is bad. . After doing some more research, it seems that multicollinearity does not fundamentally worsen a model&#39;s peformance, but rather makes interpretation of coefficients/analysis of the features much harder. . I actually found a slight decrease in peformance when removing the correlated features, so for optimal peformance this code can be commented out. I have kept the code in case I ever want to dig deeper into interpreting the influence of certain features on the model. . # for i , r in df.corr().iterrows(): # k=0 # for j in range(len(r)): # if i!= r.index[k]: # if r.values[k] &gt;=0.5: # corr_features.append([i, r.index[k], r.values[k]]) # k += 1 # feat =[] # for i in corr_features: # if i[2] &gt;= 0.8: # feat.append(i[0]) # feat.append(i[1]) # df.drop(list(set(feat)), axis=1, inplace=True) . . Visualise the distributions of each feature . At this stage, I thought it would be useful to have a look at what each feature&#39;s distribution looks like. . df.hist(figsize=(30,20)) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-10T19:49:24.110565 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ . Removing outliers . I was conflicted about including this code. On one hand, removing outliers definitely improves the model&#39;s peformance. However, I am also of the opinion that almost all data holds value, so throwing some away doesn&#39;t sit great with me. . In the end I became a sucker for peformance, and decided to remove the outliers from the datasets. Beyond just peformance gains, there is rationale for doing this. Regressions are designed to summarise the data, and having outliers greatly swing the summary of the vast majority of the dataset isn&#39;t always logical. Sometimes new, outlying data points won&#39;t be described well by a model that has omitted outliers, but it should boost peformance in the general case. . In summary, this issue is something that needs to be considered in a case by case basis. As long as there is explained reason behind the decision to remove outliers (or not), then there is good justification to go either way based on the circumstances. . In terms of implementation, as the price follows a somewhat gaussian distribution, I used a standard deviation cut-off approach to removing outliers. In short, each price was normalized, and if this value was greater than or less than 3 then I removed that row from the dataset. I&#39;ve also visualised the outliers, where blue is low outliers and red is high outliers. . zscore = [] outlier =[] threshold = 3 price_mean = np.mean(df[&#39;price&#39;]) price_std = np.std(df[&#39;price&#39;]) for i in df[&#39;price&#39;]: z = (i-price_mean)/price_std zscore.append(z) if np.abs(z) &gt; threshold: outlier.append(i) plt.figure(figsize = (10,6)) sns.distplot(zscore, kde=False) plt.axvspan(xmin = -3 ,xmax= min(zscore),alpha=0.2, color=&#39;blue&#39;, label=&#39;Lower Outliers&#39;) plt.axvspan(xmin = 3 ,xmax= max(zscore),alpha=0.2, color=&#39;red&#39;, label=&#39;Upper Outliers&#39;) plt.show() dj=[] for i in df.price: if i in set(outlier): dj.append(0.0) else: dj.append(i) df[&#39;P&#39;] = dj df = df.drop(df[df[&#39;P&#39;] == 0.0].index) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-10T19:49:28.235079 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ After the outliers were removed, I split the dataset into a feature set and a price set, and removed the redundant features appropriatley. . X = df.drop([&#39;price&#39;,&#39;P&#39;], axis=1) Y = df[&#39;price&#39;] . . Preparing the training and test datasets . The first step was to easily split the dataframes into train and test sets, on a 70:30 ratio. . x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3) . Next, to deal with categorical values (only zipcode in this case), I employed one-hot encoding to categorical features in the feature sets. . train_zip = pd.get_dummies(x_train.zipcode, prefix=&quot;zipcode&quot;) x_train = x_train.drop(&#39;zipcode&#39;,axis=1) x_train = x_train.join(train_zip) test_zip = pd.get_dummies(x_test.zipcode, prefix=&quot;zipcode&quot;) x_test = x_test.drop(&#39;zipcode&#39;,axis=1) x_test = x_test.join(test_zip) . The final stage before model selection was to scale the data. I went for the StandardScaler, which implements a basic scaling based off the mean and standard deviation of the datasets. . It is important to note that the scaling fit to the training set is the same scaling applied to the test set. This is to avoid the model learning anything about the test set before we let it attack it. . scaler = StandardScaler() scaler.fit(x_train) scaler.transform(x_train) scaler.transform(x_test) . array([[ 0.69322379, -1.47939619, -0.8889659 , ..., -0.08235545, -0.1159735 , -0.12065708], [ 0.69322379, 0.55967872, 0.23632775, ..., -0.08235545, -0.1159735 , -0.12065708], [-1.45976583, -1.47939619, 0.23632775, ..., -0.08235545, -0.1159735 , -0.12065708], ..., [-0.38327102, -0.45985874, -0.27843424, ..., -0.08235545, -0.1159735 , -0.12065708], [-1.45976583, -0.12001292, -1.06853403, ..., -0.08235545, -0.1159735 , -0.12065708], [ 1.7697186 , -0.12001292, 1.110226 , ..., -0.08235545, -0.1159735 , -0.12065708]]) . . . Model selection . To start I defined a set of models which I wanted to include in my model evaluation. These included the ones I was most familiar with, or had heard good things about. . lr = linear_model.LinearRegression() ridge = linear_model.Ridge() lasso = linear_model.Lasso() e_net = linear_model.ElasticNet() r_forest = RandomForestRegressor(n_jobs=-1) k_neighbours = KNeighborsRegressor() d_tree = DecisionTreeRegressor() models = [lr, ridge, lasso, e_net, r_forest, k_neighbours, d_tree] model_names = [&#39;Linear&#39;, &#39;Ridge&#39;, &#39;Lasso&#39;, &#39;Elastic Net&#39;, &#39;Random Forest&#39;, &#39;KNN&#39;, &#39;Decision Tree&#39;] . Next, I created a set of cross-validation sets via the sklearn KFold validator class. This allowed me to generate a preliminary R2 score for each model via cross-validation, without having to fully test each model (which would have taken an age!). . for i in range(len(models)): kfold = KFold(n_splits=7) result = cross_val_score(models[i], x_train, y_train, cv=kfold, scoring=&#39;r2&#39;) print(model_names[i] + &#39;: &#39; + str(result.mean()) ) . Linear: 0.834841451524125 Ridge: 0.834788741056295 Lasso: 0.8348479388140282 Elastic Net: 0.6154086881998547 Random Forest: 0.868900272979691 KNN: 0.4468420436933553 Decision Tree: 0.742939379397493 . . Hyperparameter tuning . Random Forest Regression came out top, with the highest R2 score. As such, in the intertest of time, I made the assumption that once tuned, this would still be the highest peforming model. . In this step, I chose the three most prudent parameters which I thought would affect the peformance of the model; number of trees, criterion (function which measures the quality of a split) and number of features considered when looking for optimal splits. . parameters = { &#39;n_estimators&#39;: [10, 50, 100], &#39;criterion&#39;: [&#39;mse&#39;, &#39;mae&#39;], &#39;max_features&#39;: [&#39;auto&#39;, &#39;sqrt&#39;, &#39;log2&#39;], } . Next, I used GridSearchCV to find the optimal set of my selected parameters; 100, mse and auto respectivley. Admittedly the first of these, number of estimators, was always going to be the largest value, so this could have been omitted from the search. If I wanted to optimise efficiency I could have plotted the peformance as a function of the number of estimators and found the point at which peformance gains become minimal. . grid = GridSearchCV(r_forest, param_grid = parameters,n_jobs=-1, scoring=&#39;r2&#39;, verbose=2) grid.fit(x_train, y_train) print(grid.best_score_) print(grid.best_params_) . Fitting 5 folds for each of 18 candidates, totalling 90 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers. [Parallel(n_jobs=-1)]: Done 25 tasks | elapsed: 13.6s [Parallel(n_jobs=-1)]: Done 90 out of 90 | elapsed: 23.0min finished 0.8686275134918929 {&#39;criterion&#39;: &#39;mse&#39;, &#39;max_features&#39;: &#39;auto&#39;, &#39;n_estimators&#39;: 100} . . Final model . Now that I have selected the optimal model and tuned its parameters, its a simple process to train and use the model on the dataset. I&#39;ve printed the R2 score here for completeness. . Note that I have included an extra parameter &#39;n_jobs&#39; which defines how many concurrent processes to run; -1 uses all available CPU cores so shoud speed it up! . r_forest = RandomForestRegressor(criterion=&#39;mse&#39;, max_features=&#39;auto&#39;, n_estimators=100,n_jobs=-1) r_forest.fit(x_train,y_train) predictions = r_forest.predict(x_test) score = r2_score(y_test,predictions) print(score) . 0.8701934934737463 . Finally, I can use the model to make a new price prediction of a new dummy data point (for ease I have made a new dummy point that is the average values across all the testing set data points). . new_pred = r_forest.predict([np.array(x_test).mean(axis=0)]) print(new_pred) . [568417.02] . I did consider visualising some of the decision trees in the regressor at this point, however the depth of the trees, in addition to the number of trees makes this fairly difficult to interpret, so I have left this out. . Feature importance . As an added bonus, I have taken a look at how significant each feature was in the model. Firstly, I plotted all feature importances, and saw that the zipcode features (which were numerous, at the end of the feature list) had low significance. . feature_importances = r_forest.feature_importances_ plt.bar([x for x in range(len(feature_importances))], feature_importances) . &lt;BarContainer object of 87 artists&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-10T19:50:38.833666 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ Removing these from the chart allows us a better view of feature significance. . importances_no_zips = feature_importances[0:17] relevant_features = x_train.columns.tolist()[0:17] print(relevant_features) fig = plt.figure(figsize=[30,10]) plt.bar(relevant_features, importances_no_zips) . [&#39;bedrooms&#39;, &#39;bathrooms&#39;, &#39;sqft_living&#39;, &#39;sqft_lot&#39;, &#39;floors&#39;, &#39;waterfront&#39;, &#39;view&#39;, &#39;condition&#39;, &#39;grade&#39;, &#39;sqft_above&#39;, &#39;sqft_basement&#39;, &#39;yr_renovated&#39;, &#39;lat&#39;, &#39;long&#39;, &#39;sqft_living15&#39;, &#39;sqft_lot15&#39;, &#39;age&#39;] . &lt;BarContainer object of 17 artists&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-10T19:52:32.665137 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ This shows a relatively high importance for the living space feature, the grade feature, and , surpisingly, the latitude feature. This being said, geographical location is a big driver of house prices, and tree based regression methods would use longitude and latitude effectivley to predict pricing. . Linear regressions and other regression models may focus on other features, and their importances could be derived in a similar way to the above if different models were chosen. . . Conclusion . In this notebook I have experimented with regression models, and found an optimal model to use on the housing dataset. The process included data manipulation/feature selection, oulier removal, correlated feature removal, feature scaling, cross-validation to select the best type of model, hyperparameter tuning, utilisation of the tuned model and finally, an analysis of feature significance. . Final R2 scores sat at roughly 0.87. To improve this I could look into further feature selection, or perhaps dimensionality reduction. There is also the prospect of utilising a neural network, but that may be overkill here and I cannot imagine there would be a significant peformance gain. . I welcome any feedback, comments or suggestions for improvement! .",
            "url": "https://ah161652.github.io/data_science_projects/regression/2021/06/09/Regression.html",
            "relUrl": "/regression/2021/06/09/Regression.html",
            "date": " • Jun 9, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a diligent and inquisitive BSc Computer Science graduate with a strong professional interest in Machine Learning, Deep Learning and Data Science. . Alongside full time work , I am currently undertaking courses in Machine Learning, Mathematics for Machine Learning, Deep Learning and Statistics in order to build upon my university education in these fields. . This website is designed for me to share my data science projects that I work on in my spare time. . For more about me, please see my LinkedIn Page and my Github Page .",
          "url": "https://ah161652.github.io/data_science_projects/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ah161652.github.io/data_science_projects/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}