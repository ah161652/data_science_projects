{
  
    
        "post0": {
            "title": "Football pass probability model",
            "content": "Introduction . The aim of this project was to build a proof-of-concept model to predict the probability of a football pass reaching a teammate successfully, based on a dataset of football match event data. The dataset used is the Metrica Sports Sample Data, which can be found at: https://github.com/metrica-sports/sample-data. The Metrica Sports repository also included player tracking data that aligns to the event data sets, which has not been used in this model, but I outline how I think this could be used to build on this model at the end of this notebook. . In short, I approached this problem as a binary classification problem, where a pass can be classified as successful, or not. The probability of the pass being successful can then be simply derived as the probability of a data point belonging to either class, which is easy to derive from almost all classification models. This greatly simplifies the ask and allowed me to use familiar classification modelling techniques as opposed to more complex statistical probability models. . I broadly split the problem into the following stages: . Data input and cleaning | Data exploration | Feature selection and engineering | Model selection and training | Model evaluation | Thought on how this model could be built upon | All in all, I created a model with a balanced accuracy score of ~70% and an ROC AUC score of ~0.75. This was pretty good, although far from perfect, and could definitley be improved on if I worked on this project full time. . This notebook includes the full code for the project, but I have made the code snippets collapsable in case you are just interested in the explanations and outputs. . Thanks for reading! . Libraries . The imported libraries are fairly standard, including various sklearn packages, pandas and matplotlib. The only rogue library I utilised was the imbalanced-learn library (https://github.com/scikit-learn-contrib/imbalanced-learn), which helped with dealing with class imbalance. I also filtered out warnings as they were getting annoying without actually telling me much. . import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn import linear_model from sklearn import naive_bayes from sklearn import neighbors from sklearn import tree from sklearn import ensemble from sklearn import svm from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score from sklearn import metrics from imblearn.over_sampling import SMOTE from imblearn.under_sampling import RandomUnderSampler from sklearn.model_selection import GridSearchCV import warnings warnings.filterwarnings(action=&#39;ignore&#39;) pd.set_option(&quot;max_columns&quot;, 50) . . Data cleaning . The first step was to read in the dataset and quickly take a look at the dataframe. . df = pd.read_csv(&quot;./Sample_Game_1/Sample_Game_1_RawEventsData.csv&quot;) df . . Team Type Subtype Period Start Frame Start Time [s] End Frame End Time [s] From To Start X Start Y End X End Y . 0 Away | SET PIECE | KICK OFF | 1 | 1 | 0.04 | 0 | 0.00 | Player19 | NaN | NaN | NaN | NaN | NaN | . 1 Away | PASS | NaN | 1 | 1 | 0.04 | 3 | 0.12 | Player19 | Player21 | 0.45 | 0.39 | 0.55 | 0.43 | . 2 Away | PASS | NaN | 1 | 3 | 0.12 | 17 | 0.68 | Player21 | Player15 | 0.55 | 0.43 | 0.58 | 0.21 | . 3 Away | PASS | NaN | 1 | 45 | 1.80 | 61 | 2.44 | Player15 | Player19 | 0.55 | 0.19 | 0.45 | 0.31 | . 4 Away | PASS | NaN | 1 | 77 | 3.08 | 96 | 3.84 | Player19 | Player21 | 0.45 | 0.32 | 0.49 | 0.47 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1740 Home | PASS | NaN | 2 | 143361 | 5734.44 | 143483 | 5739.32 | Player12 | Player13 | 0.60 | 0.33 | 0.19 | 0.95 | . 1741 Home | PASS | NaN | 2 | 143578 | 5743.12 | 143593 | 5743.72 | Player13 | Player4 | 0.09 | 0.88 | 0.14 | 0.69 | . 1742 Home | BALL LOST | INTERCEPTION | 2 | 143598 | 5743.92 | 143618 | 5744.72 | Player4 | NaN | 0.13 | 0.69 | 0.07 | 0.61 | . 1743 Away | RECOVERY | BLOCKED | 2 | 143617 | 5744.68 | 143617 | 5744.68 | Player16 | NaN | 0.05 | 0.62 | NaN | NaN | . 1744 Away | BALL OUT | NaN | 2 | 143622 | 5744.88 | 143630 | 5745.20 | Player16 | NaN | 0.05 | 0.63 | 0.03 | 1.01 | . 1745 rows × 14 columns . Next, I thought it would be useful to look at what unique values were under the &#39;Type&#39; column and the &#39;Subtype&#39; column - i.e. what different types of events were in the dataset. This through up a significant number of subtypes. . print(&#39;Types: &#39;,df.Type.unique(),&#39; n&#39;) print(&#39;Subtypes: &#39;,df.Subtype.unique()) . . Types: [&#39;SET PIECE&#39; &#39;PASS&#39; &#39;BALL LOST&#39; &#39;RECOVERY&#39; &#39;CHALLENGE&#39; &#39;BALL OUT&#39; &#39;SHOT&#39; &#39;FAULT RECEIVED&#39; &#39;CARD&#39;] Subtypes: [&#39;KICK OFF&#39; nan &#39;INTERCEPTION&#39; &#39;HEAD-INTERCEPTION&#39; &#39;AERIAL-LOST&#39; &#39;AERIAL-WON&#39; &#39;HEAD&#39; &#39;CORNER KICK&#39; &#39;CROSS&#39; &#39;HEAD-ON TARGET-GOAL&#39; &#39;THROW IN&#39; &#39;GROUND-LOST&#39; &#39;GROUND-WON&#39; &#39;TACKLE-WON&#39; &#39;THEFT&#39; &#39;TACKLE-LOST&#39; &#39;OFF TARGET-OUT&#39; &#39;GOAL KICK&#39; &#39;CROSS-INTERCEPTION&#39; &#39;GOAL KICK-INTERCEPTION&#39; &#39;DEEP BALL&#39; &#39;ON TARGET-SAVED&#39; &#39;SAVED&#39; &#39;HEAD-FORCED&#39; &#39;AERIAL-FAULT-LOST&#39; &#39;AERIAL-FAULT-WON&#39; &#39;FREE KICK&#39; &#39;HEAD-CLEARANCE&#39; &#39;CLEARANCE&#39; &#39;GROUND-FAULT-WON&#39; &#39;GROUND-FAULT-LOST&#39; &#39;HEAD-OFF TARGET-OUT&#39; &#39;DRIBBLE-WON&#39; &#39;OFFSIDE&#39; &#39;TACKLE-FAULT-LOST&#39; &#39;TACKLE-FAULT-WON&#39; &#39;GROUND&#39; &#39;FORCED&#39; &#39;YELLOW&#39; &#39;THROUGH BALL-DEEP BALL&#39; &#39;HEAD-ON TARGET-SAVED&#39; &#39;BLOCKED&#39; &#39;TACKLE-ADVANTAGE-LOST&#39; &#39;TACKLE-ADVANTAGE-WON&#39; &#39;GROUND-ADVANTAGE-LOST&#39; &#39;END HALF&#39; &#39;HEAD-WOODWORK-OUT&#39; &#39;WOODWORK-GOAL&#39; &#39;ON TARGET-GOAL&#39; &#39;WOODWORK&#39; &#39;REFEREE HIT&#39; &#39;OFF TARGET&#39;] . To narrow the dataset down to just the passes and attempted passes, I created two new dataframes; one for all datapoints under the &#39;pass&#39; type, and one for all datapoints under the &#39;ball lost&#39; type. . I then looked at the subtypes of both the &#39;pass&#39; dataframe and the &#39;ball lost&#39; dataframe, which still included many subtypes that were not relevant, specifically in the &#39;ball lost&#39; dataframe. . df_pass = df[df[&#39;Type&#39;] ==&#39;PASS&#39;] df_lost = df[df[&#39;Type&#39;] ==&#39;BALL LOST&#39;] print(&#39;Pass Subtypes: &#39;,df_pass.Subtype.unique(),&#39; n&#39;) print(&#39;Ball Lost Subtypes: &#39;,df_lost.Subtype.unique()) . . Pass Subtypes: [nan &#39;HEAD&#39; &#39;CROSS&#39; &#39;GOAL KICK&#39; &#39;DEEP BALL&#39; &#39;THROUGH BALL-DEEP BALL&#39; &#39;HEAD-CLEARANCE&#39; &#39;CLEARANCE&#39;] Ball Lost Subtypes: [&#39;INTERCEPTION&#39; &#39;HEAD-INTERCEPTION&#39; nan &#39;THEFT&#39; &#39;HEAD&#39; &#39;CROSS-INTERCEPTION&#39; &#39;GOAL KICK-INTERCEPTION&#39; &#39;HEAD-FORCED&#39; &#39;OFFSIDE&#39; &#39;FORCED&#39; &#39;CLEARANCE&#39; &#39;END HALF&#39; &#39;GOAL KICK&#39; &#39;HEAD-CLEARANCE&#39; &#39;DEEP BALL&#39; &#39;WOODWORK&#39; &#39;REFEREE HIT&#39;] . The &#39;ball lost&#39; data still needed trimming to remove all the irrelevant subtypes. To do this I wrote some code which only kept the various &#39;interception&#39; subtypes, as well as the two &#39;clearance&#39; subtypes, the &#39;goal kick&#39; subtype and the &#39;deep ball&#39; subtype. . condition = (df_lost.Subtype != &#39;&#39;) &amp; (df_lost.Subtype != &#39; &#39;) &amp; (df_lost.Subtype != &#39;THEFT&#39;) &amp; (df_lost.Subtype != &#39;HEAD&#39;) &amp; (df_lost.Subtype != &#39;HEAD-FORCED&#39;) &amp; (df_lost.Subtype != &#39;OFFSIDE&#39;) &amp; (df_lost.Subtype != &#39;FORCED&#39;) &amp; (df_lost.Subtype != &#39;END HALF&#39;) &amp; (df_lost.Subtype != &#39;WOODWORK&#39;) &amp; (df_lost.Subtype != &#39;REFEREE HIT&#39;) df_lost_trimmed = df_lost[condition] df_lost_trimmed = df_lost_trimmed.dropna(subset=[&#39;Subtype&#39;]) . . So, now I had what I believed to be all the types of events which constitute either a successful pass, or a failed pass. I have assumed that set pieces do not count as passes, offsides are discounted and clearances do count as passes. . print(&#39;Pass Subtypes: &#39;,df_pass.Subtype.unique(),&#39; n&#39;) print(&#39;Ball Lost Subtypes: &#39;,df_lost_trimmed.Subtype.unique()) . . Pass Subtypes: [nan &#39;HEAD&#39; &#39;CROSS&#39; &#39;GOAL KICK&#39; &#39;DEEP BALL&#39; &#39;THROUGH BALL-DEEP BALL&#39; &#39;HEAD-CLEARANCE&#39; &#39;CLEARANCE&#39;] Ball Lost Subtypes: [&#39;INTERCEPTION&#39; &#39;HEAD-INTERCEPTION&#39; &#39;CROSS-INTERCEPTION&#39; &#39;GOAL KICK-INTERCEPTION&#39; &#39;CLEARANCE&#39; &#39;GOAL KICK&#39; &#39;HEAD-CLEARANCE&#39; &#39;DEEP BALL&#39;] . With the data trimmed down to just pass related data, I put the two dataframes back together into a master &#39;pass&#39; dataframe and did some additionally cleaning. This included: . Renaming the ‘type’ column to &#39;pass_success&#39;, where any &#39;pass&#39; type was converted to 1 and any &#39;ball lost&#39; type to 0. This now is the dependent variable. | Fill any NaN values appropriately. For &#39;subtype&#39; NaN values, I relabelled these as &#39;standard&#39; to represent passes/attempted passes without a subtype. | Standardised the subtypes so that they are independent of if the pass was successful or not. For example, I ensured that datapoints labelled as &#39;cross-interception&#39; and &#39;cross&#39; were both just labelled as &#39;cross&#39; subtypes. This, combined with the above bullet point, allowed me to simplify the data even further to the point of having 6 subtypes of pass/attempted pass: &#39;STANDARD&#39;, &#39;HEAD&#39;, &#39;CROSS&#39;, &#39;GOAL KICK&#39;, &#39;DEEP BALL&#39;, &#39;CLEARANCE&#39;. | Changing the &#39;Team&#39; column to a binary feature of &#39;home_team&#39; where 1 represents the home team and 0 the away team. | Changing the &#39;Period&#39; column values from 1 and 2, to 0 and 1 respectively, and renaming the column to &#39;first_half&#39; | . After all this, the dataframe is clean, and is just representing successful and failed passes! See below for what it now looks like. . pass_data = pd.concat([df_pass, df_lost_trimmed]) pass_data.rename(columns={&#39;Type&#39;: &#39;pass_sucess&#39;}, inplace=True) pass_data[&quot;pass_sucess&quot;].replace({&quot;PASS&quot;: 1, &quot;BALL LOST&quot;: 0}, inplace=True) pass_data.dropna(subset = [&quot;End X&quot;], inplace=True) pass_data[&quot;Subtype&quot;].fillna(&quot;STANDARD&quot;, inplace=True) pass_data[&quot;Subtype&quot;].replace({&quot;INTERCEPTION&quot;: &quot;STANDARD&quot;}, inplace=True) pass_data[&quot;Subtype&quot;].replace({&quot;HEAD-INTERCEPTION&quot;: &quot;HEAD&quot;}, inplace=True) pass_data[&quot;Subtype&quot;].replace({&quot;THROUGH BALL-DEEP BALL&quot;: &quot;DEEP BALL&quot;}, inplace=True) pass_data[&quot;Subtype&quot;].replace({&quot;CROSS-INTERCEPTION&quot;: &quot;CROSS&quot;}, inplace=True) pass_data[&quot;Subtype&quot;].replace({&quot;HEAD-CLEARANCE&quot;: &quot;CLEARANCE&quot;}, inplace=True) pass_data[&quot;Subtype&quot;].replace({&quot;GOAL KICK-INTERCEPTION&quot;: &quot;GOAL KICK&quot;}, inplace=True) pass_data[&quot;Team&quot;].replace({&quot;Away&quot;: 0}, inplace=True) pass_data[&quot;Team&quot;].replace({&quot;Home&quot;: 1}, inplace=True) pass_data.rename(columns={&#39;Team&#39;: &#39;home_team&#39;}, inplace=True) pass_data[&quot;Period&quot;].replace({2: 0}, inplace=True) pass_data.rename(columns={&#39;Period&#39;: &#39;first_half&#39;}, inplace=True) pass_data . . home_team pass_sucess Subtype first_half Start Frame Start Time [s] End Frame End Time [s] From To Start X Start Y End X End Y . 1 0 | 1 | STANDARD | 1 | 1 | 0.04 | 3 | 0.12 | Player19 | Player21 | 0.45 | 0.39 | 0.55 | 0.43 | . 2 0 | 1 | STANDARD | 1 | 3 | 0.12 | 17 | 0.68 | Player21 | Player15 | 0.55 | 0.43 | 0.58 | 0.21 | . 3 0 | 1 | STANDARD | 1 | 45 | 1.80 | 61 | 2.44 | Player15 | Player19 | 0.55 | 0.19 | 0.45 | 0.31 | . 4 0 | 1 | STANDARD | 1 | 77 | 3.08 | 96 | 3.84 | Player19 | Player21 | 0.45 | 0.32 | 0.49 | 0.47 | . 5 0 | 1 | STANDARD | 1 | 191 | 7.64 | 217 | 8.68 | Player21 | Player22 | 0.40 | 0.73 | 0.32 | 0.98 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1712 1 | 0 | STANDARD | 0 | 140619 | 5624.76 | 140681 | 5627.24 | Player7 | NaN | 0.72 | 0.11 | 0.42 | 0.17 | . 1732 0 | 0 | STANDARD | 0 | 142886 | 5715.44 | 142938 | 5717.52 | Player27 | NaN | 0.72 | 0.08 | 0.85 | 0.32 | . 1734 1 | 0 | CLEARANCE | 0 | 142943 | 5717.72 | 143008 | 5720.32 | Player2 | NaN | 0.85 | 0.32 | 0.48 | 0.34 | . 1737 0 | 0 | STANDARD | 0 | 143229 | 5729.16 | 143247 | 5729.88 | Player18 | NaN | 0.68 | 0.30 | 0.77 | 0.23 | . 1742 1 | 0 | STANDARD | 0 | 143598 | 5743.92 | 143618 | 5744.72 | Player4 | NaN | 0.13 | 0.69 | 0.07 | 0.61 | . 958 rows × 14 columns . Data exploration . Firstly, I wanted to get a feel for the distribution of all the features. The important things to note from this visualisation are: . The classes are substantially imbalanced, with many more passes being successful than failed. This caused issues with the model and are addressed later in the project. | The &#39;start time&#39; and &#39;end time&#39; features, are carbon copies of the &#39;start frame&#39; and &#39;end frame&#39; features, albeit with different scales. As such, one set of these can be removed, which is handled later. | It is interesting that &#39;standard&#39; passes are by far the most common. | There are several features which are categorical. This is handled later. | . pass_data.hist(figsize=(30,20)) plt.show() . . pass_data[&#39;Subtype&#39;].value_counts().plot(kind=&#39;bar&#39;) . . &lt;AxesSubplot:&gt; . pass_data[&#39;From&#39;].value_counts().plot(kind=&#39;bar&#39;) . . &lt;AxesSubplot:&gt; . pass_data[&#39;To&#39;].value_counts().plot(kind=&#39;bar&#39;) . . &lt;AxesSubplot:&gt; . Feature selection and engineering . Firstly, I dealt with the duplicated data by taking out the &#39;start frame&#39; and &#39;end frame&#39; features, as their information was captured by the time columns. . I also removed the &#39;To&#39; column. This was a feature that I had to really think about omitting. On one hand, it holds some valuable information, as there could be a relationship between pass success and which player was being passed to. However, the dataset did not include &#39;To&#39; data for passes that were incomplete/failed/intercepted. As such, I would not have this data for the failed pass class, and this would cause major issues in the model as the model would essentially know if a pass was successful just from seeing that the &#39;To&#39; column had a value. As a result, I decided to just omit the feature for integrity and, to some degree, simplicity. . I also questioned the utility of the &#39;end time&#39; feature. Any information about the game, such as how long the game had been played for, would be captured in the start time feature. The only additional information that the end time feature carries is how long the pass took to reach a teammate or be intercepted. As such, I created a new column called &#39;pass_length&#39; which was equal to the &#39;end time&#39; minus the &#39;start time’ and removed the &#39;end time&#39; column from the data. . After all this feature selection, the dataframe was close to being in its final state. . pass_data.drop([&#39;Start Frame&#39;, &#39;End Frame&#39;, &#39;To&#39;], axis=1, inplace=True) new_column = pass_data[&quot;End Time [s]&quot;] - pass_data[&quot;Start Time [s]&quot;] pass_data[&quot;pass_length&quot;] = new_column pass_data.drop([&#39;End Time [s]&#39;], axis=1, inplace=True) pass_data . . home_team pass_sucess Subtype first_half Start Time [s] From Start X Start Y End X End Y pass_length . 1 0 | 1 | STANDARD | 1 | 0.04 | Player19 | 0.45 | 0.39 | 0.55 | 0.43 | 0.08 | . 2 0 | 1 | STANDARD | 1 | 0.12 | Player21 | 0.55 | 0.43 | 0.58 | 0.21 | 0.56 | . 3 0 | 1 | STANDARD | 1 | 1.80 | Player15 | 0.55 | 0.19 | 0.45 | 0.31 | 0.64 | . 4 0 | 1 | STANDARD | 1 | 3.08 | Player19 | 0.45 | 0.32 | 0.49 | 0.47 | 0.76 | . 5 0 | 1 | STANDARD | 1 | 7.64 | Player21 | 0.40 | 0.73 | 0.32 | 0.98 | 1.04 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1712 1 | 0 | STANDARD | 0 | 5624.76 | Player7 | 0.72 | 0.11 | 0.42 | 0.17 | 2.48 | . 1732 0 | 0 | STANDARD | 0 | 5715.44 | Player27 | 0.72 | 0.08 | 0.85 | 0.32 | 2.08 | . 1734 1 | 0 | CLEARANCE | 0 | 5717.72 | Player2 | 0.85 | 0.32 | 0.48 | 0.34 | 2.60 | . 1737 0 | 0 | STANDARD | 0 | 5729.16 | Player18 | 0.68 | 0.30 | 0.77 | 0.23 | 0.72 | . 1742 1 | 0 | STANDARD | 0 | 5743.92 | Player4 | 0.13 | 0.69 | 0.07 | 0.61 | 0.80 | . 958 rows × 11 columns . I then split the dataframe into two; one for the dependent variable &#39;y&#39; (the class variable; successful pass or failed pass), and a feature dataframe &#39;x&#39;. . The feature dataframe still had a couple of categorical variables, so I utilised one-hot encoding to convert these to multiple binary features. This was done for the &#39;Subtype&#39; feature, and the &#39;From&#39; feature, to give binary features for every subtype of pass and every passing player. . I did not utilise normalization as this reduced the accuracy of my models in the model selection stage. Regardless, with normalization enabled or disabled, the random forest classifier was the highest performing model, and as random forest classifiers do not require normalization, I believe this step is fine to be omitted. . The final form of the feature dataframe can be seen below. The y dataframe is merely a single column of 1&#39;s or 0&#39;s, so I have not visualised it here. . x = pass_data.drop([&#39;pass_sucess&#39;], axis=1) y = pass_data[&#39;pass_sucess&#39;] cat_features_string = [&#39;Subtype&#39;,&#39;From&#39;] cat_features = [x.Subtype, x.From] for i in range(len(cat_features_string)): x_temp = pd.get_dummies(cat_features[i], prefix=cat_features_string[i]) x = x.drop(cat_features_string[i],axis=1) x = x.join(x_temp) x . . home_team first_half Start Time [s] Start X Start Y End X End Y pass_length Subtype_CLEARANCE Subtype_CROSS Subtype_DEEP BALL Subtype_GOAL KICK Subtype_HEAD Subtype_STANDARD From_Player1 From_Player10 From_Player11 From_Player12 From_Player13 From_Player14 From_Player15 From_Player16 From_Player17 From_Player18 From_Player19 From_Player2 From_Player20 From_Player21 From_Player22 From_Player23 From_Player24 From_Player25 From_Player26 From_Player27 From_Player28 From_Player3 From_Player4 From_Player5 From_Player6 From_Player7 From_Player8 From_Player9 . 1 0 | 1 | 0.04 | 0.45 | 0.39 | 0.55 | 0.43 | 0.08 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 1 | 0.12 | 0.55 | 0.43 | 0.58 | 0.21 | 0.56 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 1 | 1.80 | 0.55 | 0.19 | 0.45 | 0.31 | 0.64 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 1 | 3.08 | 0.45 | 0.32 | 0.49 | 0.47 | 0.76 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 0 | 1 | 7.64 | 0.40 | 0.73 | 0.32 | 0.98 | 1.04 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1712 1 | 0 | 5624.76 | 0.72 | 0.11 | 0.42 | 0.17 | 2.48 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 1732 0 | 0 | 5715.44 | 0.72 | 0.08 | 0.85 | 0.32 | 2.08 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1734 1 | 0 | 5717.72 | 0.85 | 0.32 | 0.48 | 0.34 | 2.60 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1737 0 | 0 | 5729.16 | 0.68 | 0.30 | 0.77 | 0.23 | 0.72 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1742 1 | 0 | 5743.92 | 0.13 | 0.69 | 0.07 | 0.61 | 0.80 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | . 958 rows × 42 columns . Model selection and training . Before selecting a model, I split the data into training and test sets, at an 80/20% split. . Additionally, I had several issues with my models returning a high accuracy, but only because the dataset is imbalanced. The large number of successful passes compared to the low number of failed passes meant that my models were prone to labelling almost all the passes as successful, resulting in a high accuracy. In reality, this wasn&#39;t very useful, as the predictions for failed passes were very inaccurate. To solve this issue, I used random under sampling to reduce the number of successful pass datapoints, and SMOTE oversampling to boost the number of failed pass samples (albeit by creating synthetic datapoints). This resulted in an even distribution of successful and failed pass datapoints, which greatly improved the ROC AUC score of my models. The trade-offs of this approach are discussed in the model evaluation section. . x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2) rus = RandomUnderSampler() x_train, y_train = rus.fit_resample(x_train, y_train) x_train, y_train = SMOTE().fit_resample(x_train, y_train) y_train.hist(figsize=(10,10)) plt.suptitle(&quot;New distribution of failed/sucessful passes&quot;) plt.show() . . Next, I defined a list of models which I wanted to test on the data. This is my usual approach, as I think it is quick and easy to test multiple models on a dataset and find the best performing model. I chose a variety of classification models which I am familiar with. I also tested some bagging, stacking, and boosting models after I had finished the project to see if these would improve performance, but they did not, so I have not included these here. . Once I had defined which models I would be testing on the data, I created a set of cross-validation sets via the sklearn KFold validator class. This allowed me to generate preliminary scores for each model via cross-validation, without having to fully test each model. . I chose to evaluate my model at this stage with 3 scores: F1 score, accuracy, and balanced accuracy. Accuracy is always handy to see, but I did not want to rely on it, especially as the dataset was imbalanced to start with. I also included F1 score and a &#39;balanced&#39; accuracy metric, which together should shed some greater light on the performance of the models. Ultimately I was guided by the balanced accuracy scores for which model was best performing, as this shows the average of recall attained across the two classes, which is a good indicator of model performance for imbalanced datasets. Once I had employed under sampling/oversampling, accuracy may have been OK to use, but I still put the majority of my trust in the balanced accuracy score. . The scores of each model can be seen below. Consistently, the Naive Bayes classifier and the Random Forest classifier came out on top. I chose to go ahead with the Random Forest classifier as it allowed for better predictions of pass probability than the Naive Bayes classifier. Note that the scores are lower than for the completed model due to the small sample sizes used in this preliminary testing. . log_regression = linear_model.LogisticRegression() n_bayes = naive_bayes.GaussianNB() sgd = linear_model.SGDClassifier() knn = neighbors.KNeighborsClassifier() rand_forest = ensemble.RandomForestClassifier(n_jobs=-1) svc = svm.SVC() models = [log_regression, n_bayes, sgd, knn, rand_forest, svc] model_names = [&#39;Logistic Regression&#39;, &#39;Naive Bayes&#39;, &#39;Stochastic Gradient Descent&#39;,&#39;K-Nearest-Neighbours&#39;, &#39;Random Forest&#39;, &#39;Support Vector Classifier&#39;] print(&#39;F1:&#39;) for i in range(len(models)): kfold = KFold(n_splits=7) result = cross_val_score(models[i], x_train, y_train, cv=kfold, scoring=&#39;f1&#39;) print(model_names[i] + &#39;: &#39; + str(result.mean()) ) print(&#39; nAccuracy:&#39;) for i in range(len(models)): kfold = KFold(n_splits=7) result = cross_val_score(models[i], x_train, y_train, cv=kfold, scoring=&#39;accuracy&#39;) print(model_names[i] + &#39;: &#39; + str(result.mean()) ) print(&#39; nBalanced Accuracy:&#39;) for i in range(len(models)): kfold = KFold(n_splits=7) result = cross_val_score(models[i], x_train, y_train, cv=kfold, scoring=&#39;balanced_accuracy&#39;) print(model_names[i] + &#39;: &#39; + str(result.mean()) ) . . F1: Logistic Regression: 0.3361118694969627 Naive Bayes: 0.5081979272647476 Stochastic Gradient Descent: 0.3827493261455525 K-Nearest-Neighbours: 0.2783027179419409 Random Forest: 0.40219618114354955 Support Vector Classifier: 0.09316770186335403 Accuracy: Logistic Regression: 0.40552220888355345 Naive Bayes: 0.6240096038415367 Stochastic Gradient Descent: 0.07346938775510203 K-Nearest-Neighbours: 0.3525810324129651 Random Forest: 0.5858343337334935 Support Vector Classifier: 0.08979591836734693 Balanced Accuracy: Logistic Regression: 0.40513538748832867 Naive Bayes: 0.62328264639189 Stochastic Gradient Descent: 0.35714285714285715 K-Nearest-Neighbours: 0.3529811924769907 Random Forest: 0.5782246231826063 Support Vector Classifier: 0.08857543017206883 . Next, I did some hyperparameter tuning on my chosen model to ensure that I was using the optimal parameters in the model. The code below outputs the optimal parameters to be used. . parameters = { &#39;criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;], &#39;max_features&#39;: [&#39;auto&#39;, &#39;sqrt&#39;, &#39;log2&#39;], } grid = GridSearchCV(rand_forest, param_grid = parameters, scoring=&#39;balanced_accuracy&#39;, verbose=1) grid.fit(x_train, y_train) print(grid.best_params_) . . Fitting 5 folds for each of 6 candidates, totalling 30 fits {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;sqrt&#39;} . At this point I had found the optimal model, and the optimal parameters for the model. As such, I was ready to build the model with the optimised parameters and train it on the data. Once training was complete, I ran the model on the test dataset, and derived final balanced accuracy and F1 scores. These scores hovered around 0.7 and 0.8 respectively, which is pretty good! . rand_forest = ensemble.RandomForestClassifier(criterion = &#39;entropy&#39;, max_features=&#39;sqrt&#39;, n_jobs=-1) rand_forest.fit(x_train,y_train) predictions = rand_forest.predict(x_test) accuracy = metrics.balanced_accuracy_score(y_test,predictions) f1 = metrics.f1_score(y_test,predictions) print(&#39;Balanced Accuracy: &#39;+str(accuracy)) print(&#39;F1 Score: &#39;+str(f1)) . . Balanced Accuracy: 0.7435047951176983 F1 Score: 0.8014705882352942 . Now that the model is built, we can use it to predict the probability of a pass being successful. The model I chose was a random forest classifier, so the probability is derived from the proportion of sub-trees in the forest that predict the datapoint will be a successful pass. Although not as statistically hardcore as some other probabilistic models, this is a simple and effective approach, and allows me to output the probabilities of various pass datapoints. Below is an example, which shows a datapoint&#39;s probabilities being a 22% chance of pass failure, and a 78% chance of pass success. . probs = rand_forest.predict_proba(x_test) print(probs[42]) . . [0.22 0.78] . Model evaluation . The balanced accuracy scores and F1 scores gave me an indication of model performance, but I wanted to see a better representation of how well my model was classifying the passes. To do this I plotted a confusion matrix, and a ROC curve. . The confusion matrix shows that the model is mostly predicting the correct classes. Before I employed oversampling and under sampling, this confusion matrix was very skewed, being very accurate at predicting successful passes, but terrible at predicting failed passes. . The ROC curve shows the relationship between the false positive rate and the true positive rate. The area under the curve (AUC) can be used as a metric of how well the model distinguishes between the two classes. Here, the AUC score is quite high, landing between ~ 0.7 and 0.8. . c_matrix = metrics.plot_confusion_matrix(rand_forest, x_test,y_test) metrics.plot_roc_curve(rand_forest, x_test,y_test) plt.show() . . The final piece of evaluation I did was to visualise which features were most significant to the model. Luckily, most classification models have an easy way to view this, and the graph below shows the results. . Pass length was the most influential feature, which makes sense. The passer&#39;s coordinates, the receivers coordinates and the time at which the ball was passed were also all major contributors. Home/away team had some impact, as did which half the pass was played in. . As for types of passes, a pass being a standard pass had the highest influence on classification, which makes sense as these are likely to be the simplest pass to pull off. . Which player was the passer had some influence, of varying degrees, but this was generally less important that the other features. . feature_importances = rand_forest.feature_importances_ feature_strings = x_train.columns.tolist() fig = plt.figure(figsize=[15,10]) plt.xticks(rotation=90) plt.bar(feature_strings, feature_importances) plt.show() . . Finally, I tested all the above code on the second game&#39;s data in the Metrica repository, and I got similar results, which proves the model is somewhat robust! . Further work . Although my model performs well, there were a load of ideas which I thought would benefit the model which I have not had time to implement. These are listed below: . The dataset also came with tracking data of the players for the game. This could have been used to derive 3 extra features. Firstly, a &#39;pressure&#39; metric could have been derived based on the number of opposition players near the passer at the time of the pass. This would likely have a good influence on pass success rates. Secondly, a fatigue metric could have been derived from how far a player has run up to the point of a pass. The fatigue of a player would likely affect the probability of a pass being successful. Finally, the distribution of teammates and opposition on the pitch at the time of the pass could have been utilised, as I am sure this would have an influence on pass success. | The position of players (e.g., left winger) could have had an influence on pass probability, if we had this data. That being said, I already had player data, so this could be redundant. Nonetheless, it would have been interesting to experiment with. | Rather than using x and y coordinates, splitting the pitch into a grid, and using an &#39;area of pitch&#39; feature could have been useful. Even if this did not improve performance, the visualisation of this feature could be very useful. | Using a &#39;game state&#39; feature(s), which accounts for the score, any yellow/red cards, the importance of the game etc. could have been useful. | I would have loved to have been able to use more data. If I had historic game data, I could have done a variety of interesting things, such as creating player and team profiles across multiple games, as well as having much more data to train and test on. | I approached this problem as a binary classification problem, with associated class probabilities. This worked, but I am sure there are more pure statistical methods for approaching this problem. The research and implementation of these methods would be of interest. | Finally, taking the next step into production of this model is an interesting and complex challenge. The world of ML ops is new to me, but utilising a model on real-world data, monitoring the model&#39;s performance, and making iterative improvements is an exciting challenge. To do this properly, I think a constant feed of both new and historic data is necessary and adapting the model to account for multiple games&#39; data is a must. | .",
            "url": "https://ah161652.github.io/data_science_projects/classification/2021/07/07/Football-pass-probability-model.html",
            "relUrl": "/classification/2021/07/07/Football-pass-probability-model.html",
            "date": " • Jul 7, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Classification - heart disease",
            "content": "Summary . The aim of this project is to experiment with data manipulation, adequately prepare a dataset for use and apply various classification models to the manipulated data in order to build an accurate classification model. I also employed hyperparameter tuning to the best model to further increase performance. . All in all, I developed a classification model with accuracy and F1 scores both around 0.83. The recall score, which is particulary important for medical classification models, sat at 0.91. . Libraries . import numpy as np import pandas as pd import sklearn as sk from sklearn import preprocessing import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn import linear_model from sklearn import naive_bayes from sklearn import neighbors from sklearn import tree from sklearn import ensemble from sklearn import svm from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score import warnings from sklearn import metrics from sklearn.model_selection import GridSearchCV warnings.filterwarnings(action=&#39;ignore&#39;) pd.set_option(&quot;max_columns&quot;, 25) . . . Data input and exploration . Firstly, I read in the dataset and had a little explore of the features. This was a relativley small dataset of 303 entries, with 13 features contributing to the classification of either no heart disease (0) or heart disease (1). It was also good to see that the dataset was complete, with no null values across all entries, so there was no need for dealing with any null values. . df = pd.read_csv(&quot;../projects/Classification/heart.csv&quot;) . df.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 303 entries, 0 to 302 Data columns (total 14 columns): # Column Non-Null Count Dtype -- -- 0 age 303 non-null int64 1 sex 303 non-null int64 2 cp 303 non-null int64 3 trestbps 303 non-null int64 4 chol 303 non-null int64 5 fbs 303 non-null int64 6 restecg 303 non-null int64 7 thalach 303 non-null int64 8 exang 303 non-null int64 9 oldpeak 303 non-null float64 10 slope 303 non-null int64 11 ca 303 non-null int64 12 thal 303 non-null int64 13 target 303 non-null int64 dtypes: float64(1), int64(13) memory usage: 33.3 KB . Next, I wanted to visualise the distribution of the features. As you can see, there are a significant number of categorical features, which I will deal with later. For completeness, see below for the full description of the features: . age: Self explanatory | sex: 0 is female, 1 is male | cp: Chest pain type in increasing severity | trestbps: Resting blood pressure upon hospital admission | chol: Cholesterol levels | fbs: Was the patient fasting (1 = true, 0 = false) | restecg: Category of resting electrocardiograph results | thalach: Maximum heart rate | exang: Does the patient suffer from exercise induced angina (1 = true, 0 = false) | oldpeak: ST depression induced by exercise | slope: The slope of the peak exercise ST segment | ca: Number of major vessels (0-3) colored by fluoroscopy | thal: Category of a blood disorder called thalassemia, where 3 is normal, and 0-2 are abnormal. | target: Heart disease or not (1 = true, 0 = false) | . df.hist(figsize=(30,20)) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-26T09:59:09.487589 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ I also wanted to get a feel for if there was any inbalance in the dependent variable. As seen below, the dataset is slightly inbalanced, but not the to the degree that I should be concerned that my models will not be effective. . sns.countplot(x=&quot;target&quot;, data=df) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-26T10:18:43.947607 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ . Data cleaning . As the dataset was already in very good condition, the only additional cleaning I could think to do was to check for any correlated features and remove them from the model. I wrote some code to remove pairs of features which showed a high degree of correlation (&gt;80%). I did this as my understanding is that highly correlated features are redundant in many classification algorithms, and are detrimental for some. . It turned out that none of the features were highly correlated, so this code is redundant, but I have kept it in for reference. I have also printed a matrix of correlation, which proves my code is correct! . corr_features =[] for i , r in df.corr().iterrows(): k=0 for j in range(len(r)): if i!= r.index[k]: if r.values[k] &gt;=0.5: corr_features.append([i, r.index[k], r.values[k]]) k += 1 feat =[] for i in corr_features: print(i[2]) if i[2] &gt;= 0.8: feat.append(i[0]) feat.append(i[1]) df.drop(list(set(feat)), axis=1, inplace=True) . df.corr() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . age 1.000000 | -0.098447 | -0.068653 | 0.279351 | 0.213678 | 0.121308 | -0.116211 | -0.398522 | 0.096801 | 0.210013 | -0.168814 | 0.276326 | 0.068001 | -0.225439 | . sex -0.098447 | 1.000000 | -0.049353 | -0.056769 | -0.197912 | 0.045032 | -0.058196 | -0.044020 | 0.141664 | 0.096093 | -0.030711 | 0.118261 | 0.210041 | -0.280937 | . cp -0.068653 | -0.049353 | 1.000000 | 0.047608 | -0.076904 | 0.094444 | 0.044421 | 0.295762 | -0.394280 | -0.149230 | 0.119717 | -0.181053 | -0.161736 | 0.433798 | . trestbps 0.279351 | -0.056769 | 0.047608 | 1.000000 | 0.123174 | 0.177531 | -0.114103 | -0.046698 | 0.067616 | 0.193216 | -0.121475 | 0.101389 | 0.062210 | -0.144931 | . chol 0.213678 | -0.197912 | -0.076904 | 0.123174 | 1.000000 | 0.013294 | -0.151040 | -0.009940 | 0.067023 | 0.053952 | -0.004038 | 0.070511 | 0.098803 | -0.085239 | . fbs 0.121308 | 0.045032 | 0.094444 | 0.177531 | 0.013294 | 1.000000 | -0.084189 | -0.008567 | 0.025665 | 0.005747 | -0.059894 | 0.137979 | -0.032019 | -0.028046 | . restecg -0.116211 | -0.058196 | 0.044421 | -0.114103 | -0.151040 | -0.084189 | 1.000000 | 0.044123 | -0.070733 | -0.058770 | 0.093045 | -0.072042 | -0.011981 | 0.137230 | . thalach -0.398522 | -0.044020 | 0.295762 | -0.046698 | -0.009940 | -0.008567 | 0.044123 | 1.000000 | -0.378812 | -0.344187 | 0.386784 | -0.213177 | -0.096439 | 0.421741 | . exang 0.096801 | 0.141664 | -0.394280 | 0.067616 | 0.067023 | 0.025665 | -0.070733 | -0.378812 | 1.000000 | 0.288223 | -0.257748 | 0.115739 | 0.206754 | -0.436757 | . oldpeak 0.210013 | 0.096093 | -0.149230 | 0.193216 | 0.053952 | 0.005747 | -0.058770 | -0.344187 | 0.288223 | 1.000000 | -0.577537 | 0.222682 | 0.210244 | -0.430696 | . slope -0.168814 | -0.030711 | 0.119717 | -0.121475 | -0.004038 | -0.059894 | 0.093045 | 0.386784 | -0.257748 | -0.577537 | 1.000000 | -0.080155 | -0.104764 | 0.345877 | . ca 0.276326 | 0.118261 | -0.181053 | 0.101389 | 0.070511 | 0.137979 | -0.072042 | -0.213177 | 0.115739 | 0.222682 | -0.080155 | 1.000000 | 0.151832 | -0.391724 | . thal 0.068001 | 0.210041 | -0.161736 | 0.062210 | 0.098803 | -0.032019 | -0.011981 | -0.096439 | 0.206754 | 0.210244 | -0.104764 | 0.151832 | 1.000000 | -0.344029 | . target -0.225439 | -0.280937 | 0.433798 | -0.144931 | -0.085239 | -0.028046 | 0.137230 | 0.421741 | -0.436757 | -0.430696 | 0.345877 | -0.391724 | -0.344029 | 1.000000 | . . Preparing the train/test sets and dealing with categorical variables . First, I needed to deal with all the categorical features in the dataset. These were the sex, fbs, restecg, exang, slope and thal features. Note that the cp and ca features look categorical, but they are ordinal, as they have a natural ordering (increasing severity of chest pain and number of highlighted blood vessels) so I have kept these as continious variables, albeit accross a small range. . I used one-hot encoding to split each categorical feature into a series of new binary features which are more easily interpreted by many models. . x = df.drop([&#39;target&#39;], axis=1) y = df[&#39;target&#39;] cat_features_string = [&#39;sex&#39;,&#39;fbs&#39;,&#39;restecg&#39;,&#39;exang&#39;,&#39;slope&#39;,&#39;thal&#39;] cat_features = [x.sex,x.fbs,x.restecg,x.exang,x.slope,x.thal] for i in range(len(cat_features_string)): x_temp = pd.get_dummies(cat_features[i], prefix=cat_features_string[i]) x = x.drop(cat_features_string[i],axis=1) x = x.join(x_temp) . Once this was done it was a simple matter of splitting the dataframes into train an test sets, at a 70/30 split. . x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3) . x_train.head() . age cp trestbps chol thalach oldpeak ca sex_0 sex_1 fbs_0 fbs_1 restecg_0 restecg_1 restecg_2 exang_0 exang_1 slope_0 slope_1 slope_2 thal_0 thal_1 thal_2 thal_3 . 95 53 | 0 | 142 | 226 | 111 | 0.0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | . 156 47 | 2 | 130 | 253 | 179 | 0.0 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | . 169 53 | 0 | 140 | 203 | 155 | 3.1 | 0 | 0 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | . 206 59 | 0 | 110 | 239 | 142 | 1.2 | 1 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | . 81 45 | 1 | 128 | 308 | 170 | 0.0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | . . Data normalisation . The final bit of data manipulation to do was to scale the data, via the StandardScaler class. It is important to note that the scaling fit to the training set is the same scaling applied to the test set. This is to avoid the model learning anything about the test set before we let it attack it. . scaler = StandardScaler() scaler.fit(x_train) scaler.transform(x_train) scaler.transform(x_test) . array([[-0.58533247, -0.0089799 , -0.01274095, ..., -0.24494897, 0.95389198, -0.83426614], [-1.15143869, -0.96084962, 0.10469735, ..., -0.24494897, -1.04833673, 1.19865825], [ 1.56587117, 0.94288982, 1.2790803 , ..., -0.24494897, 0.95389198, -0.83426614], ..., [ 0.66010122, -0.96084962, 2.33602496, ..., -0.24494897, -1.04833673, 1.19865825], [ 1.45264993, -0.96084962, 1.74883348, ..., 4.0824829 , -1.04833673, -0.83426614], [ 1.45264993, -0.96084962, -1.06968561, ..., -0.24494897, 0.95389198, -0.83426614]]) . . . Model selection . The first thing to do was to decide which classification models I was going to experiment with. I have taken a selection of models which I am familiar with, or have heard of. . log_regression = linear_model.LogisticRegression() n_bayes = naive_bayes.GaussianNB() sgd = linear_model.SGDClassifier() knn = neighbors.KNeighborsClassifier() dec_tree = tree.DecisionTreeClassifier() rand_forest = ensemble.RandomForestClassifier(n_jobs=-1) svc = svm.SVC() models = [log_regression, n_bayes, sgd, knn, dec_tree, rand_forest, svc] model_names = [&#39;Logistic Regression&#39;, &#39;Naive Bayes&#39;, &#39;Stochastic Gradient Descent&#39;, &#39;K-Nearest-Neighbours&#39;, &#39;Decision Tree&#39;, &#39;Random Forest&#39;, &#39;Support Vector Classifier&#39;] . Next, I created a set of cross-validation sets via the sklearn KFold validator class. This allowed me to generate preliminary scores for each model via cross-validation, without having to fully test each model. . I have chosen to evaluate my model with 4 scores; accuracy, F1 score, recall and precision. This is because accuracy is not always the best indicator of an effective classification model. The F1 score gives a balanced view of both precision and recall, but I have included recall and precision scores also, for completeness. . As this is a medical classification model, we want to see a high recall rate in order to keep the false negative rates low, which can be dangerous in medical settings. If we can get a model that also has high accuracy and F1 scores, thereby reducing the number of false positives too, then even better! . print(&#39;Recall:&#39;) for i in range(len(models)): kfold = KFold(n_splits=7) result = cross_val_score(models[i], x_train, y_train, cv=kfold, scoring=&#39;recall&#39;) print(model_names[i] + &#39;: &#39; + str(result.mean()) ) print(&#39;Precision:&#39;) for i in range(len(models)): kfold = KFold(n_splits=7) result = cross_val_score(models[i], x_train, y_train, cv=kfold, scoring=&#39;precision&#39;) print(model_names[i] + &#39;: &#39; + str(result.mean()) ) print(&#39;F1:&#39;) for i in range(len(models)): kfold = KFold(n_splits=7) result = cross_val_score(models[i], x_train, y_train, cv=kfold, scoring=&#39;f1&#39;) print(model_names[i] + &#39;: &#39; + str(result.mean()) ) print(&#39;Accuracy:&#39;) for i in range(len(models)): kfold = KFold(n_splits=7) result = cross_val_score(models[i], x_train, y_train, cv=kfold, scoring=&#39;accuracy&#39;) print(model_names[i] + &#39;: &#39; + str(result.mean()) ) . Recall: Logistic Regression: 0.8800924013330027 Naive Bayes: 0.8129856463503079 Stochastic Gradient Descent: 0.4858560090702948 K-Nearest-Neighbours: 0.7308858226151458 Decision Tree: 0.7930175714010301 Random Forest: 0.8700867323987624 Support Vector Classifier: 0.9254385964912281 Precision: Logistic Regression: 0.8315786638542056 Naive Bayes: 0.8493772309561782 Stochastic Gradient Descent: 0.7714285714285715 K-Nearest-Neighbours: 0.6587292093871041 Decision Tree: 0.7772203091763021 Random Forest: 0.8021119986456121 Support Vector Classifier: 0.6293854696028609 F1: Logistic Regression: 0.8525817392164293 Naive Bayes: 0.8255877541591826 Stochastic Gradient Descent: 0.570452918009309 K-Nearest-Neighbours: 0.683342810808534 Decision Tree: 0.7841796462486118 Random Forest: 0.8277212205783634 Support Vector Classifier: 0.7402477037445304 Accuracy: Logistic Regression: 0.8394777265745006 Naive Bayes: 0.8109062980030721 Stochastic Gradient Descent: 0.6125960061443932 K-Nearest-Neighbours: 0.6231950844854071 Decision Tree: 0.7540706605222735 Random Forest: 0.8204301075268816 Support Vector Classifier: 0.6367127496159755 . The results were interesting. If I ran the above code without changing anything then logistic regression came out on top, with an accuracy of 84 and an F1 score of 85. . If I removed the normalisation of the variables, I saw similar results. . If I removed the one-hot encoding but kept the normalisation the logistic regression classifier maintained its peformance, and was rivalled by naive bayes for both accuracy and F1 score. . If I removed normalisation and one hot encoding there was a slight hit to the peformance of the logistic regression model, and the naive bayes model came out on top with 82 accuracy and 84 F1. . As such, it was a toss up between naive bayes with no normalisaton or one-hot encoding, vs logistic regression with the above two steps enabled. I decided to stick with the code I had already written and make use of the logistic regression as this seemed to be the most consistent with some further repeated tests. . I believe the Naive Bayes classifier faltered with the one hot encoding enabled as I was using a gaussian Naive Bayes algorithm, which isn&#39;t best suited with alot of binary features. If I changed the Naive Bayes implementation to a Bernoulli Naive Bayes implementation then I received slightly better results, but still not as good as logisitic regression. I put this down to Bernoulli handling binary features well, but there are still plenty of non binary features in the dataset. . Final decision: Logistic regression classifier. . . Hyperparameter tuning . I made the assumption that once tuned, the logistic regression classifier would still be the highest peforming model. . In this step, I chose the most prudent parameters which I thought would affect the peformance of the model. . parameters = { &#39;penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;, &#39;elasticnet&#39;, &#39;none&#39;], &#39;solver&#39;: [&#39;newton-cg&#39;, &#39;lbfgs&#39;, &#39;liblinear&#39;, &#39;sag&#39;, &#39;saga&#39;], } . Next, I used GridSearchCV to find the optimal set of my selected parameters; this turned out to be the &#39;l1&#39; and &#39;liblinear&#39;. The scoring I was comapring against was recall, as for medical classification I want as high a recall score as possible. . grid = GridSearchCV(log_regression, param_grid = parameters,n_jobs=-1, scoring=&#39;recall&#39;, verbose=2) grid.fit(x_train, y_train) print(grid.best_score_) print(grid.best_params_) . Fitting 5 folds for each of 20 candidates, totalling 100 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers. [Parallel(n_jobs=-1)]: Done 34 tasks | elapsed: 0.2s [Parallel(n_jobs=-1)]: Done 85 out of 100 | elapsed: 0.3s remaining: 0.1s 0.8833333333333334 {&#39;penalty&#39;: &#39;l1&#39;, &#39;solver&#39;: &#39;liblinear&#39;} [Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 0.5s finished . . Final model . Now that I have selected the optimal model and tuned its parameters, its a simple process to train and use the model on the dataset. I&#39;ve printed the F1 and accuracy scores here for completeness. . log_regression = linear_model.LogisticRegression(penalty=&#39;l1&#39;, solver=&#39;liblinear&#39;, max_iter=1000000, n_jobs=-1) log_regression.fit(x_train,y_train) predictions = log_regression.predict(x_test) accuracy = metrics.accuracy_score(y_test,predictions) f1 = metrics.f1_score(y_test,predictions) recall = metrics.recall_score(y_test,predictions) precision = metrics.precision_score(y_test,predictions) print(&#39;Accuracy: &#39;+str(accuracy)) print(&#39;F1 Score: &#39;+str(f1)) print(&#39;Recall: &#39;+str(recall)) print(&#39;Precision: &#39;+str(precision)) . Accuracy: 0.8131868131868132 F1 Score: 0.8210526315789474 Recall: 0.8666666666666667 Precision: 0.78 . . Model evaluation . The model looks pretty good. 81% accuracy, F1 score of 0.82 and a nice high recall of 0.87, keeping the false negatives of heart disease down. . I&#39;ve visualised the confusion matrix and ROC plot below to see what the split of predictions is. . c_matrix = metrics.plot_confusion_matrix(log_regression, x_test,y_test) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-26T15:18:34.286067 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ metrics.plot_roc_curve(log_regression, x_test,y_test) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-26T15:10:16.988678 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ On the surface, the model looks to be peforming strongly. The accuracy is high, the F1 score is strong, and the AUC is close to 1. . This being said, theres still roughly 13% of positive cases being mislabelled as negative cases. This could be down to the small sample size, or could indicate that the model needs further tweaking in order to favour recall over precision. . . Increasing recall . In order to boost the recall value, I am going to experiment with the &#39;C&#39; parameter for logistic regression. This is the inverse of regularisation strength, with smaller values normally leading to a greater recall. The default is 1, so let&#39;s lower it as see what happens. . log_regression = linear_model.LogisticRegression(penalty=&#39;l1&#39;, solver=&#39;liblinear&#39;, max_iter=1000000, n_jobs=-1, C=0.25) log_regression.fit(x_train,y_train) predictions = log_regression.predict(x_test) accuracy = metrics.accuracy_score(y_test,predictions) f1 = metrics.f1_score(y_test,predictions) recall = metrics.recall_score(y_test,predictions) precision = metrics.precision_score(y_test,predictions) print(&#39;Accuracy: &#39;+str(accuracy)) print(&#39;F1 Score: &#39;+str(f1)) print(&#39;Recall: &#39;+str(recall)) print(&#39;Precision: &#39;+str(precision)) c_matrix = metrics.plot_confusion_matrix(log_regression, x_test,y_test) plt.show() . Accuracy: 0.8241758241758241 F1 Score: 0.8367346938775511 Recall: 0.9111111111111111 Precision: 0.7735849056603774 . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-26T15:23:34.200498 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ With the reduced C value I can get the recall to 91 and the false negative rate to under 10% of true negatives. That is pretty good for a normal classification model, but this certainly wouldn&#39;t be up to scratch for a true medical model! . To improve the model further I think that I would need to do some in depth feature engineering. This would require a level of medical domain knowledge which I do not posess, so for now I am going to accept my model as adequate. . . Feature analysis . Finally, I want to take a look at which features had the most signficance on the model. The simplest thing to do here is to take a look at the coefficients asscoiated with each feature. The higher the coefficient, the higher its influence on the model. . coefs = log_regression.coef_[0] features = x_train.columns.tolist() fig = plt.figure(figsize=[30,10]) plt.bar(features, coefs) . &lt;BarContainer object of 23 artists&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-26T15:34:20.462626 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ This led to some interesting insight. Firsly, age did not play a big part in the model, which may be of surprise. . Chest pain severity was a big factor, as expected, as was being female and having an abnormal thalassemia condition. . Maximum heartrate and number of major blood vessels visible under fluoroscopy were strongly weighted against heart disease, which does make sense, as was being a man and having a normal thalassemia condition. . These results seem reasonable, however I am not a doctor, nor do I have any background in medicine, so my interpretation of these results is somewhat limitted. That is why data science is so interesting, as if I was truly tasked with this problem then I would have to become clued up on the subject matter in order to analyse in depth. . . Conclusion . In this notebook I have experimented with classification models, and found an optimal model to use on the medical dataset. The process included data manipulation and cleaning, correlated feature removal, feature scaling, cross-validation to select the best type of model, hyperparameter tuning, utilisation of the tuned model, evaluation alongside visualisation and finally, an analysis of feature significance. . Final accuracy and F1 scores sat at roughly 0.83. I think this is close to the maximal peformance I can get out of this dataset without doing some more complex feature engineering, increasing the sample size, or using a more in depth feature set. . I welcome any feedback, comments or suggestions for improvement! .",
            "url": "https://ah161652.github.io/data_science_projects/classification/2021/06/26/Classification.html",
            "relUrl": "/classification/2021/06/26/Classification.html",
            "date": " • Jun 26, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Regression - predicting house prices",
            "content": "Summary . The aim of this project is to experiment with data manipulation, adequately prepare a dataset for use and apply various regression models to the manipulated data in order to build an accurate regression model. I also employed hyperparameter tuning to the best model to further increase performance. . All in all, I developed a regression model with an R2 score of ~0.87 . Full code can be found here. . . Library imports . import numpy as np import pandas as pd import sklearn as sk from sklearn import preprocessing import matplotlib.pyplot as plt from sklearn import linear_model from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.neighbors import KNeighborsRegressor from sklearn.tree import DecisionTreeRegressor import seaborn as sns from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score import warnings from sklearn.svm import LinearSVR from sklearn.preprocessing import StandardScaler from sklearn import svm, datasets from sklearn.model_selection import GridSearchCV from sklearn.inspection import permutation_importance warnings.filterwarnings(action=&#39;ignore&#39;) pd.set_option(&quot;max_columns&quot;, 25) . . . Data input and data cleaning . The first stage is to use pandas to read in the dataset into a dataframe, investigate the data, and replace any null values with 0&#39;s. . df = pd.read_csv(&#39;../projects/regression/kc_house_data.csv&#39;) df.fillna(0, inplace=True) df . id date price bedrooms bathrooms sqft_living sqft_lot floors waterfront view condition grade sqft_above sqft_basement yr_built yr_renovated zipcode lat long sqft_living15 sqft_lot15 . 0 7129300520 | 20141013T000000 | 221900.0 | 3 | 1.00 | 1180 | 5650 | 1.0 | 0 | 0 | 3 | 7 | 1180 | 0 | 1955 | 0 | 98178 | 47.5112 | -122.257 | 1340 | 5650 | . 1 6414100192 | 20141209T000000 | 538000.0 | 3 | 2.25 | 2570 | 7242 | 2.0 | 0 | 0 | 3 | 7 | 2170 | 400 | 1951 | 1991 | 98125 | 47.7210 | -122.319 | 1690 | 7639 | . 2 5631500400 | 20150225T000000 | 180000.0 | 2 | 1.00 | 770 | 10000 | 1.0 | 0 | 0 | 3 | 6 | 770 | 0 | 1933 | 0 | 98028 | 47.7379 | -122.233 | 2720 | 8062 | . 3 2487200875 | 20141209T000000 | 604000.0 | 4 | 3.00 | 1960 | 5000 | 1.0 | 0 | 0 | 5 | 7 | 1050 | 910 | 1965 | 0 | 98136 | 47.5208 | -122.393 | 1360 | 5000 | . 4 1954400510 | 20150218T000000 | 510000.0 | 3 | 2.00 | 1680 | 8080 | 1.0 | 0 | 0 | 3 | 8 | 1680 | 0 | 1987 | 0 | 98074 | 47.6168 | -122.045 | 1800 | 7503 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 21608 263000018 | 20140521T000000 | 360000.0 | 3 | 2.50 | 1530 | 1131 | 3.0 | 0 | 0 | 3 | 8 | 1530 | 0 | 2009 | 0 | 98103 | 47.6993 | -122.346 | 1530 | 1509 | . 21609 6600060120 | 20150223T000000 | 400000.0 | 4 | 2.50 | 2310 | 5813 | 2.0 | 0 | 0 | 3 | 8 | 2310 | 0 | 2014 | 0 | 98146 | 47.5107 | -122.362 | 1830 | 7200 | . 21610 1523300141 | 20140623T000000 | 402101.0 | 2 | 0.75 | 1020 | 1350 | 2.0 | 0 | 0 | 3 | 7 | 1020 | 0 | 2009 | 0 | 98144 | 47.5944 | -122.299 | 1020 | 2007 | . 21611 291310100 | 20150116T000000 | 400000.0 | 3 | 2.50 | 1600 | 2388 | 2.0 | 0 | 0 | 3 | 8 | 1600 | 0 | 2004 | 0 | 98027 | 47.5345 | -122.069 | 1410 | 1287 | . 21612 1523300157 | 20141015T000000 | 325000.0 | 2 | 0.75 | 1020 | 1076 | 2.0 | 0 | 0 | 3 | 7 | 1020 | 0 | 2008 | 0 | 98144 | 47.5941 | -122.299 | 1020 | 1357 | . 21613 rows × 21 columns . Next, I wanted simplify the &#39;date&#39; feature to just capture the year the house was recorded, and then use this in conjunction with the &#39;year_built&#39; feature to create one new feature; &#39;age&#39;. . d =[] for i in df[&#39;date&#39;].values: d.append(i[:4]) df[&#39;date&#39;] = d df[&#39;date&#39;]=df[&#39;date&#39;].astype(float) df[&#39;age&#39;] = df[&#39;date&#39;] - df[&#39;yr_built&#39;] . The next stage in data cleaning was to remove any features which were obviously redundant. Here, I removed the &#39;date&#39; and &#39;yr_built&#39; features which I had already simplified, alongside the &#39;id&#39; feature. . df = df.drop([&quot;date&quot;, &quot;id&quot;, &#39;yr_built&#39;], axis=1) . . Removing correlated features . Next, I wrote some code to remove pairs of features which showed a high degree of correlation (&gt;80%). I did this as my understanding is that for regression problems, multicollinearity is bad. . After doing some more research, it seems that multicollinearity does not fundamentally worsen a model&#39;s peformance, but rather makes interpretation of coefficients/analysis of the features much harder. . I actually found a slight decrease in peformance when removing the correlated features, so for optimal peformance this code can be commented out. I have kept the code in case I ever want to dig deeper into interpreting the influence of certain features on the model. . # for i , r in df.corr().iterrows(): # k=0 # for j in range(len(r)): # if i!= r.index[k]: # if r.values[k] &gt;=0.5: # corr_features.append([i, r.index[k], r.values[k]]) # k += 1 # feat =[] # for i in corr_features: # if i[2] &gt;= 0.8: # feat.append(i[0]) # feat.append(i[1]) # df.drop(list(set(feat)), axis=1, inplace=True) . . Visualise the distributions of each feature . At this stage, I thought it would be useful to have a look at what each feature&#39;s distribution looks like. . df.hist(figsize=(30,20)) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-10T19:49:24.110565 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ . Removing outliers . I was conflicted about including this code. On one hand, removing outliers definitely improves the model&#39;s peformance. However, I am also of the opinion that almost all data holds value, so throwing some away doesn&#39;t sit great with me. . In the end I became a sucker for peformance, and decided to remove the outliers from the datasets. Beyond just peformance gains, there is rationale for doing this. Regressions are designed to summarise the data, and having outliers greatly swing the summary of the vast majority of the dataset isn&#39;t always logical. Sometimes new, outlying data points won&#39;t be described well by a model that has omitted outliers, but it should boost peformance in the general case. . In summary, this issue is something that needs to be considered in a case by case basis. As long as there is explained reason behind the decision to remove outliers (or not), then there is good justification to go either way based on the circumstances. . In terms of implementation, as the price follows a somewhat gaussian distribution, I used a standard deviation cut-off approach to removing outliers. In short, each price was normalized, and if this value was greater than or less than 3 then I removed that row from the dataset. I&#39;ve also visualised the outliers, where blue is low outliers and red is high outliers. . zscore = [] outlier =[] threshold = 3 price_mean = np.mean(df[&#39;price&#39;]) price_std = np.std(df[&#39;price&#39;]) for i in df[&#39;price&#39;]: z = (i-price_mean)/price_std zscore.append(z) if np.abs(z) &gt; threshold: outlier.append(i) plt.figure(figsize = (10,6)) sns.distplot(zscore, kde=False) plt.axvspan(xmin = -3 ,xmax= min(zscore),alpha=0.2, color=&#39;blue&#39;, label=&#39;Lower Outliers&#39;) plt.axvspan(xmin = 3 ,xmax= max(zscore),alpha=0.2, color=&#39;red&#39;, label=&#39;Upper Outliers&#39;) plt.show() dj=[] for i in df.price: if i in set(outlier): dj.append(0.0) else: dj.append(i) df[&#39;P&#39;] = dj df = df.drop(df[df[&#39;P&#39;] == 0.0].index) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-10T19:49:28.235079 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ After the outliers were removed, I split the dataset into a feature set and a price set, and removed the redundant features appropriatley. . X = df.drop([&#39;price&#39;,&#39;P&#39;], axis=1) Y = df[&#39;price&#39;] . . Preparing the training and test datasets . The first step was to easily split the dataframes into train and test sets, on a 70:30 ratio. . x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3) . Next, to deal with categorical values (only zipcode in this case), I employed one-hot encoding to categorical features in the feature sets. . train_zip = pd.get_dummies(x_train.zipcode, prefix=&quot;zipcode&quot;) x_train = x_train.drop(&#39;zipcode&#39;,axis=1) x_train = x_train.join(train_zip) test_zip = pd.get_dummies(x_test.zipcode, prefix=&quot;zipcode&quot;) x_test = x_test.drop(&#39;zipcode&#39;,axis=1) x_test = x_test.join(test_zip) . The final stage before model selection was to scale the data. I went for the StandardScaler, which implements a basic scaling based off the mean and standard deviation of the datasets. . It is important to note that the scaling fit to the training set is the same scaling applied to the test set. This is to avoid the model learning anything about the test set before we let it attack it. . scaler = StandardScaler() scaler.fit(x_train) scaler.transform(x_train) scaler.transform(x_test) . array([[ 0.69322379, -1.47939619, -0.8889659 , ..., -0.08235545, -0.1159735 , -0.12065708], [ 0.69322379, 0.55967872, 0.23632775, ..., -0.08235545, -0.1159735 , -0.12065708], [-1.45976583, -1.47939619, 0.23632775, ..., -0.08235545, -0.1159735 , -0.12065708], ..., [-0.38327102, -0.45985874, -0.27843424, ..., -0.08235545, -0.1159735 , -0.12065708], [-1.45976583, -0.12001292, -1.06853403, ..., -0.08235545, -0.1159735 , -0.12065708], [ 1.7697186 , -0.12001292, 1.110226 , ..., -0.08235545, -0.1159735 , -0.12065708]]) . . . Model selection . To start I defined a set of models which I wanted to include in my model evaluation. These included the ones I was most familiar with, or had heard good things about. . lr = linear_model.LinearRegression() ridge = linear_model.Ridge() lasso = linear_model.Lasso() e_net = linear_model.ElasticNet() r_forest = RandomForestRegressor(n_jobs=-1) k_neighbours = KNeighborsRegressor() d_tree = DecisionTreeRegressor() models = [lr, ridge, lasso, e_net, r_forest, k_neighbours, d_tree] model_names = [&#39;Linear&#39;, &#39;Ridge&#39;, &#39;Lasso&#39;, &#39;Elastic Net&#39;, &#39;Random Forest&#39;, &#39;KNN&#39;, &#39;Decision Tree&#39;] . Next, I created a set of cross-validation sets via the sklearn KFold validator class. This allowed me to generate a preliminary R2 score for each model via cross-validation, without having to fully test each model (which would have taken an age!). . for i in range(len(models)): kfold = KFold(n_splits=7) result = cross_val_score(models[i], x_train, y_train, cv=kfold, scoring=&#39;r2&#39;) print(model_names[i] + &#39;: &#39; + str(result.mean()) ) . Linear: 0.834841451524125 Ridge: 0.834788741056295 Lasso: 0.8348479388140282 Elastic Net: 0.6154086881998547 Random Forest: 0.868900272979691 KNN: 0.4468420436933553 Decision Tree: 0.742939379397493 . . Hyperparameter tuning . Random Forest Regression came out top, with the highest R2 score. As such, in the intertest of time, I made the assumption that once tuned, this would still be the highest peforming model. . In this step, I chose the three most prudent parameters which I thought would affect the peformance of the model; number of trees, criterion (function which measures the quality of a split) and number of features considered when looking for optimal splits. . parameters = { &#39;n_estimators&#39;: [10, 50, 100], &#39;criterion&#39;: [&#39;mse&#39;, &#39;mae&#39;], &#39;max_features&#39;: [&#39;auto&#39;, &#39;sqrt&#39;, &#39;log2&#39;], } . Next, I used GridSearchCV to find the optimal set of my selected parameters; 100, mse and auto respectivley. Admittedly the first of these, number of estimators, was always going to be the largest value, so this could have been omitted from the search. If I wanted to optimise efficiency I could have plotted the peformance as a function of the number of estimators and found the point at which peformance gains become minimal. . grid = GridSearchCV(r_forest, param_grid = parameters,n_jobs=-1, scoring=&#39;r2&#39;, verbose=2) grid.fit(x_train, y_train) print(grid.best_score_) print(grid.best_params_) . Fitting 5 folds for each of 18 candidates, totalling 90 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers. [Parallel(n_jobs=-1)]: Done 25 tasks | elapsed: 13.6s [Parallel(n_jobs=-1)]: Done 90 out of 90 | elapsed: 23.0min finished 0.8686275134918929 {&#39;criterion&#39;: &#39;mse&#39;, &#39;max_features&#39;: &#39;auto&#39;, &#39;n_estimators&#39;: 100} . . Final model . Now that I have selected the optimal model and tuned its parameters, its a simple process to train and use the model on the dataset. I&#39;ve printed the R2 score here for completeness. . Note that I have included an extra parameter &#39;n_jobs&#39; which defines how many concurrent processes to run; -1 uses all available CPU cores so shoud speed it up! . r_forest = RandomForestRegressor(criterion=&#39;mse&#39;, max_features=&#39;auto&#39;, n_estimators=100,n_jobs=-1) r_forest.fit(x_train,y_train) predictions = r_forest.predict(x_test) score = r2_score(y_test,predictions) print(score) . 0.8701934934737463 . Finally, I can use the model to make a new price prediction of a new dummy data point (for ease I have made a new dummy point that is the average values across all the testing set data points). . new_pred = r_forest.predict([np.array(x_test).mean(axis=0)]) print(new_pred) . [568417.02] . I did consider visualising some of the decision trees in the regressor at this point, however the depth of the trees, in addition to the number of trees makes this fairly difficult to interpret, so I have left this out. . Feature importance . As an added bonus, I have taken a look at how significant each feature was in the model. Firstly, I plotted all feature importances, and saw that the zipcode features (which were numerous, at the end of the feature list) had low significance. . feature_importances = r_forest.feature_importances_ plt.bar([x for x in range(len(feature_importances))], feature_importances) . &lt;BarContainer object of 87 artists&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-10T19:50:38.833666 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ Removing these from the chart allows us a better view of feature significance. . importances_no_zips = feature_importances[0:17] relevant_features = x_train.columns.tolist()[0:17] print(relevant_features) fig = plt.figure(figsize=[30,10]) plt.bar(relevant_features, importances_no_zips) . [&#39;bedrooms&#39;, &#39;bathrooms&#39;, &#39;sqft_living&#39;, &#39;sqft_lot&#39;, &#39;floors&#39;, &#39;waterfront&#39;, &#39;view&#39;, &#39;condition&#39;, &#39;grade&#39;, &#39;sqft_above&#39;, &#39;sqft_basement&#39;, &#39;yr_renovated&#39;, &#39;lat&#39;, &#39;long&#39;, &#39;sqft_living15&#39;, &#39;sqft_lot15&#39;, &#39;age&#39;] . &lt;BarContainer object of 17 artists&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-06-10T19:52:32.665137 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ This shows a relatively high importance for the living space feature, the grade feature, and , surpisingly, the latitude feature. This being said, geographical location is a big driver of house prices, and tree based regression methods would use longitude and latitude effectivley to predict pricing. . Linear regressions and other regression models may focus on other features, and their importances could be derived in a similar way to the above if different models were chosen. . . Conclusion . In this notebook I have experimented with regression models, and found an optimal model to use on the housing dataset. The process included data manipulation/feature selection, oulier removal, correlated feature removal, feature scaling, cross-validation to select the best type of model, hyperparameter tuning, utilisation of the tuned model and finally, an analysis of feature significance. . Final R2 scores sat at roughly 0.87. To improve this I could look into further feature selection, or perhaps dimensionality reduction. There is also the prospect of utilising a neural network, but that may be overkill here and I cannot imagine there would be a significant peformance gain. . I welcome any feedback, comments or suggestions for improvement! .",
            "url": "https://ah161652.github.io/data_science_projects/regression/2021/06/09/Regression.html",
            "relUrl": "/regression/2021/06/09/Regression.html",
            "date": " • Jun 9, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a diligent and inquisitive BSc Computer Science graduate with a strong professional interest in Machine Learning, Deep Learning and Data Science. . Alongside full time work , I am currently undertaking courses in Machine Learning, Mathematics for Machine Learning, Deep Learning and Statistics in order to build upon my university education in these fields. . This website is designed for me to share my data science projects that I work on in my spare time. . For more about me, please see my LinkedIn Page and my Github Page .",
          "url": "https://ah161652.github.io/data_science_projects/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ah161652.github.io/data_science_projects/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}